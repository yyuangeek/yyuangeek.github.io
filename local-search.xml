<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>使用GitLab CI和Docker自动部署SpringBoot应用</title>
    <link href="/2020/06/18/%E4%BD%BF%E7%94%A8GitLab%20CI%E5%92%8CDocker%E8%87%AA%E5%8A%A8%E9%83%A8%E7%BD%B2SpringBoot%E5%BA%94%E7%94%A8/"/>
    <url>/2020/06/18/%E4%BD%BF%E7%94%A8GitLab%20CI%E5%92%8CDocker%E8%87%AA%E5%8A%A8%E9%83%A8%E7%BD%B2SpringBoot%E5%BA%94%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<p>Docker和Spring Boot是非常流行的组合，我们将利用GitLab CI的优势，并在应用程序服务器上自动构建，推送和运行Docker镜像。</p><h2 id="GitLab-CI"><a href="#GitLab-CI" class="headerlink" title="GitLab CI"></a>GitLab CI</h2><p>Gitlab CI/CD服务是GitLab的一部分。开发人员将代码推送到GitLab存储库时，GitLab CI就会在用户指定的环境中自动构建，测试和存储最新的代码更改。</p><p><strong>选择GitLab CI的一些主要原因：</strong></p><ol><li>易于学习，使用和可扩展</li><li>维护容易</li><li>整合容易</li><li>CI完全属于GitLab存储库的一部分</li><li>良好的Docker集成</li><li>镜像托管(Container registry)-基本上是你自己的私有Docker Hub</li><li>从成本上来说，GitLab CI是一个很好的解决方案。每个月你有2000分钟的免费构建时间，对于某些项目来说，这是绰绰有余的</li></ol><h3 id="为什么GitLab-CI超越Jenkins"><a href="#为什么GitLab-CI超越Jenkins" class="headerlink" title="为什么GitLab CI超越Jenkins"></a>为什么GitLab CI超越Jenkins</h3><p>这无疑是一个广泛讨论的话题，但是在本文中，我们将不深入探讨该话题。GitLab CI和Jenkins都有优点和缺点，它们都是功能非常强大的工具。</p><h4 id="那为什么选择GitLab？"><a href="#那为什么选择GitLab？" class="headerlink" title="那为什么选择GitLab？"></a>那为什么选择GitLab？</h4><p>如前所述，Gitlab CI是GitLab存储库的一部分，这就意味着当我们有了GitLab后，就不需要再安装Gitlab CI，也不需要额外维护。并且只需要编写一个.gitlab-ci.yml文件（下文会详细说明），你便完成了所有CI工作。</p><p>对于小型项目使用Jenkins，你就必须自己配置所有内容。通常，你还需要一台专用的Jenkins服务器，这也需要额外的成本和维护。</p><h4 id="使用GitLab-CI-前提条件"><a href="#使用GitLab-CI-前提条件" class="headerlink" title="使用GitLab CI 前提条件"></a>使用GitLab CI 前提条件</h4><p>如果需要与这些前提条件有关的任何帮助，我已提供相应指南的链接。</p><ol><li>你已经在GitLab上推送了Spring Boot项目</li><li>你已在应用程序服务器上安装了Docker（<a href="https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-on-ubuntu-18-04" target="_blank" rel="noopener">指南</a>）</li><li>你具有Docker镜像的镜像托管（本文中将使用<a href="https://hub.docker.com/" target="_blank" rel="noopener">Docker Hub</a>）</li><li>你已经在服务器上生成了SSH RSA密钥（<a href="https://www.digitalocean.com/community/tutorials/how-to-set-up-ssh-keys-on-ubuntu-1604" target="_blank" rel="noopener">指南</a>）</li></ol><h4 id="你要创建什么"><a href="#你要创建什么" class="headerlink" title="你要创建什么"></a><strong>你要创建什么</strong></h4><p>你将创建Dockerfile 和.gitlab-ci.yml， 它们将自动用于：</p><ol><li>构建应用程序Jar文件</li><li>构建Docker镜像</li><li>将镜像推送到Docker存储库</li><li>在应用程序服务器上运行镜像</li></ol><h4 id="基本项目信息"><a href="#基本项目信息" class="headerlink" title="基本项目信息"></a><strong>基本项目信息</strong></h4><p>本文的Spring Boot应用程序是通过<a href="https://start.spring.io/" target="_blank" rel="noopener">Spring Initializr</a>生成的。这是一个基于Java 8或Java11构建的Maven项目。后面，我们将介绍Java 8和Java 11对Docker镜像有什么影响。</p><h3 id="Docker文件"><a href="#Docker文件" class="headerlink" title="Docker文件"></a>Docker文件</h3><p>让我们从Dockerfile开始。</p><pre><code class="hljs dockerfile"><span class="hljs-keyword">FROM</span> maven:<span class="hljs-number">3.6</span>.<span class="hljs-number">3</span>-jdk-<span class="hljs-number">11</span>-slim AS MAVEN_BUILD<span class="hljs-comment">#FROM maven:3.5.2-jdk-8-alpine AS MAVEN_BUILD FOR JAVA 8</span><span class="hljs-keyword">ARG</span> SPRING_ACTIVE_PROFILE<span class="hljs-keyword">MAINTAINER</span> Jasmin<span class="hljs-keyword">COPY</span><span class="bash"> pom.xml /build/</span><span class="hljs-keyword">COPY</span><span class="bash"> src /build/src/</span><span class="hljs-keyword">WORKDIR</span><span class="bash"> /build/</span><span class="hljs-keyword">RUN</span><span class="bash"> mvn clean install -Dspring.profiles.active=<span class="hljs-variable">$SPRING_ACTIVE_PROFILE</span> &amp;&amp; mvn package -B -e -Dspring.profiles.active=<span class="hljs-variable">$SPRING_ACTIVE_PROFILE</span></span><span class="hljs-keyword">FROM</span> openjdk:<span class="hljs-number">11</span>-slim<span class="hljs-comment">#FROM openjdk:8-alpine FOR JAVA 8</span><span class="hljs-keyword">WORKDIR</span><span class="bash"> /app</span><span class="hljs-keyword">COPY</span><span class="bash"> --from=MAVEN_BUILD /build/target/appdemo-*.jar /app/appdemo.jar</span><span class="hljs-keyword">ENTRYPOINT</span><span class="bash"> [<span class="hljs-string">"java"</span>, <span class="hljs-string">"-jar"</span>, <span class="hljs-string">"appdemo.jar"</span>]</span></code></pre><h2 id="Java版本"><a href="#Java版本" class="headerlink" title="Java版本"></a>Java版本</h2><p>让我们从Docker的角度看一下<strong>Java 8和11之间的区别。长话短说：这是Docker镜像的大小和部署时间。</strong></p><p>基于Java 8构建的Docker镜像将明显小于基于Java 11的镜像。这也意味着Java 8项目的构建和部署时间将更快。</p><blockquote><p>Java 8-构建时间：约4分钟，镜像大小为 约180 MB</p><p>Java 11-构建时间： 约14分钟，镜像大小约为480 MB</p></blockquote><p>注意： 在实际应用中，这些数字可能会有所不同。</p><h2 id="Docker镜像"><a href="#Docker镜像" class="headerlink" title="Docker镜像"></a>Docker镜像</h2><p>正如在前面示例中已经看到的那样，由于Java版本的缘故，我们在应用程序镜像大小和构建时间方面存在巨大差异。其背后的实际原因是在Dockerfile中使用了不同的Docker镜像。</p><p>如果我们再看一下Dockerfile，<strong>那么Java 11镜像很大的真正原因是因为它包含了没有经过验证/测试的open-jdk:11镜像的Alpine版本。</strong></p><p>如果你不熟悉OpenJDK镜像版本，建议你阅读<a href="https://hub.docker.com/_/openjdk" target="_blank" rel="noopener">OpenJDK Docker官方文档</a>。在这里，你可以找到有关每个OpenJDK版本的镜像的说明。</p><h3 id="备注：动态的变量"><a href="#备注：动态的变量" class="headerlink" title="备注：动态的变量"></a>备注：动态的变量</h3><p>在ENTRYPOINT 中，与环境相关的属性，我们只能写死，如下：</p><pre><code class="hljs java">ENTRYPOINT [ “ java”，“ -Dspring.profiles.active = development”，“ -jar”，“ appdemo.jar” ]</code></pre><p>为了使它动态，你希望将其简单地转换为：</p><blockquote><pre><code class="hljs java">ENTRYPOINT [ “ java”，“ -Dspring.profiles.active = $ SPRINT_ACTIVE_PROFILE”，“ -jar”，“ appdemo.jar” ]</code></pre></blockquote><p>以前，这是不可能的，<strong>但是幸运的是，这将在.gitlab-ci.yml中通过 ARG SPRING_ACTIVE_PROFILE修复。</strong></p><h3 id="gitlab-ci-yml"><a href="#gitlab-ci-yml" class="headerlink" title="gitlab-ci.yml"></a>gitlab-ci.yml</h3><p>在编写此文件之前，要准备的东西很少。基本上，我们想要实现的是，只要推送代码，就会在相应的环境上自动部署。</p><h4 id="创建-env文件和分支"><a href="#创建-env文件和分支" class="headerlink" title="创建.env文件和分支"></a>创建.env文件和分支</h4><p><strong>我们首先需要创建包含与环境相关的分支和.env文件。每个分支实际上代表我们的应用程序将运行的环境。</strong></p><p>我们将在三个不同的环境中部署我们的应用程序：开发，测试和生产（ development, QA, and production ）。这意味着我们需要创建三个分支。我们的dev，QA和prod应用程序将在不同的服务器上运行，并且将具有不同的Docker容器标签，端口和SSH密钥。<strong>这就要求我们的gitlab-ci.yml文件将要是动态的。我们可以为每个环境创建单独的.env文件来解决该问题。</strong></p><blockquote><p>.develop.env</p><p>.qa.env</p><p>.master.env</p></blockquote><p>重要说明： 命名这些文件时，有一个简单的规则：使用GitLab分支来命名，因此文件名应如下所示：。<strong>$ BRANCH_NAME.env</strong></p><p>例如，这是.develop.env文件。</p><pre><code class="hljs shell">export SPRING_ACTIVE_PROFILE='development'export DOCKER_REPO='username/demo_app:dev'export APP_NAME='demo_app_dev'export PORT='8080'export SERVER_IP='000.11.222.33'export SERVER_SSH_KEY="$DEV_SSH_PRIVATE_KEY"</code></pre><p>与.env文件有关的重要说明：</p><ul><li><strong>SPRING_ACTIVE_PROFILE：</strong>不言自明，我们要使用哪些Spring应用程序属性。 <strong>DOCKER_REPO：</strong>这是Docker镜像的存储库；在这里，我们唯一需要注意的是Docker image TAG，对于每种环境，我们将使用不同的标签，这意味着我们将使用dev，qa 和prod 标签。</li></ul><p>我们的Docker中心看起来像这样。</p><p><img src="C:%5CUsers%5CAdministrator%5CPictures%5C1-2.png" srcset="/img/loading.gif" alt="1-2"></p><p>如你所见，存在一个带有三个不同标签的存储库，每当将代码推送到GitLab分支上时，每个标签（应用程序版本）都会被更新。</p><ul><li><strong>APP_NAME：</strong> 此属性非常重要，它是对容器的命名。 如果你未设置此属性，则Docker将为你的容器随机命名。这可能是一个问题，因为你将无法以干净的方式停止运行容器。</li><li>端口：这是我们希望运行Docker容器的端口。</li><li><strong>SERVER_IP：</strong>应用程序使用的服务器IP。通常，每个环境都将位于不同的服务器上。</li><li><strong>SERVER_SSH_KEY：</strong>这是我们已经在每台服务器上生成的SSH密钥。 $DEV_SSH_PRIVATE_KEY 实际上是来自GitLab存储库的变量。</li></ul><h3 id="创建GitLab变量"><a href="#创建GitLab变量" class="headerlink" title="创建GitLab变量"></a>创建GitLab变量</h3><p>最后需要做的是创建GitLab变量。</p><p>打开你的GitLab存储库，然后转到：Settings -&gt; CI/CD。在 Variables部分中， 添加新变量：</p><ul><li><strong>DOCKER_USER：</strong>用于访问Docker Hub或其他镜像托管的用户名</li><li><strong>DOCKER_PASSWORD：</strong> 用于访问镜像托管的密码</li><li><strong>$ENV_SSH_PRIVATE_KEY：</strong> 先前在服务器上生成的SSH私钥。</li></ul><p>SSH KEY的重要说明：</p><ul><li>你需要复制完整的密钥值，包括： —– BEGIN RSA PRIVATE KEY —–和—– END RSA PRIVATE KEY —–</li></ul><p>最后，你的GitLab变量应如下所示。</p><p><img src="C:%5CUsers%5CAdministrator%5CPictures%5C2-3.png" srcset="/img/loading.gif" alt="2-3"></p><h3 id="创建gitlab-ci-yml文件"><a href="#创建gitlab-ci-yml文件" class="headerlink" title="创建gitlab-ci.yml文件"></a>创建gitlab-ci.yml文件</h3><p>最后，让我们创建将所有内容放在一起的文件。</p><pre><code class="hljs java">services:  - docker:<span class="hljs-number">19.03</span><span class="hljs-number">.7</span>-dindstages:  - build jar  - build and push docker image  - deploybuild:  image: maven:<span class="hljs-number">3.6</span><span class="hljs-number">.3</span>-jdk-<span class="hljs-number">11</span>-slim  stage: build jar  before_script:    - source .$&#123;CI_COMMIT_REF_NAME&#125;.env  script:    - mvn clean install -Dspring.profiles.active=$SPRING_ACTIVE_PROFILE &amp;&amp; mvn <span class="hljs-keyword">package</span> -B -e -Dspring.profiles.active=$SPRING_ACTIVE_PROFILE  artifacts:    paths:      - target<span class="hljs-comment">/*.jar</span><span class="hljs-comment">docker build:</span><span class="hljs-comment">  image: docker:stable</span><span class="hljs-comment">  stage: build and push docker image</span><span class="hljs-comment">  before_script:</span><span class="hljs-comment">    - source .$&#123;CI_COMMIT_REF_NAME&#125;.env</span><span class="hljs-comment">  script:</span><span class="hljs-comment">    - docker build --build-arg SPRING_ACTIVE_PROFILE=$SPRING_ACTIVE_PROFILE -t $DOCKER_REPO .</span><span class="hljs-comment">    - docker login -u $DOCKER_USER -p $DOCKER_PASSWORD docker.io</span><span class="hljs-comment">    - docker push $DOCKER_REPO</span><span class="hljs-comment">deploy:</span><span class="hljs-comment">  image: ubuntu:latest</span><span class="hljs-comment">  stage: deploy</span><span class="hljs-comment">  before_script:</span><span class="hljs-comment">    - 'which ssh-agent || ( apt-get update -y &amp;&amp; apt-get install openssh-client -y )'</span><span class="hljs-comment">    - eval $(ssh-agent -s)</span><span class="hljs-comment">    - echo "$SSH_PRIVATE_KEY" | tr -d '\r' | ssh-add -</span><span class="hljs-comment">    - mkdir -p ~/.ssh</span><span class="hljs-comment">    - chmod 700 ~/.ssh</span><span class="hljs-comment">    - echo -e "Host *\n\tStrictHostKeyChecking no\n\n" &gt; ~/.ssh/config</span><span class="hljs-comment">    - source .$&#123;CI_COMMIT_REF_NAME&#125;.env</span><span class="hljs-comment">  script:</span><span class="hljs-comment">    - ssh root@$SERVER "docker login -u $DOCKER_USER -p $DOCKER_PASSWORD docker.io; docker stop $APP_NAME; docker system prune -a -f; docker pull $DOCKER_REPO; docker container run -d --name $APP_NAME -p $PORT:8080 -e SPRING_PROFILES_ACTIVE=$SPRING_ACTIVE_PROFILE $DOCKER_REPO; docker logout"</span></code></pre><p>让我们解释一下这里发生了什么：</p><pre><code class="hljs java">services:  - docker:<span class="hljs-number">19.03</span><span class="hljs-number">.7</span>-dind</code></pre><p>这是一项服务，使我们可以在Docker中使用Docker。在Docker中运行Docker通常不是一个好主意，但是对于此用例来说，这是完全可以的，因为我们将构建镜像并将其推送到存储库中。</p><pre><code class="hljs shell">stages:  - build jar  - build and push docker image  - deploy</code></pre><p><strong>对于每个gitlab-ci.yml文件，必须首先定义执行步骤。脚本将按照步骤定义的顺序执行。</strong></p><p>在每个步骤，我们都必须添加以下部分：</p><pre><code class="hljs shell">before_script: - source .$&#123;CI_COMMIT_REF_NAME&#125;.env</code></pre><p>这只是预先加载之前创建的 env. files文件。根据正在运行的分支来自动注入变量。（这就是为什么我们必须使用分支名称来命名.env文件的原因）</p><p>这些是我们部署过程中的执行步骤。</p><p><img src="C:%5CUsers%5CAdministrator%5CPictures%5C3-2.png" srcset="/img/loading.gif" alt="3-2"></p><p>如你所见，，有三个带有绿色复选标记的圆圈，这表示所有步骤均已成功执行。</p><pre><code class="hljs shell">build:  image: maven:3.6.3-jdk-11-slim  stage: build jar  before_script:    - source .$&#123;CI_COMMIT_REF_NAME&#125;.env  script:    - mvn clean install -Dspring.profiles.active=$SPRING_ACTIVE_PROFILE &amp;&amp; mvn package -B -e -Dspring.profiles.active=$SPRING_ACTIVE_PROFILE  artifacts:    paths:      - target/*.jar</code></pre><p>这是执行第一步骤代码的一部分，构建了一个jar文件，该文件可以下载。这实际上是一个可选步骤，仅用于演示构建jar并从GitLab下载它是多么容易。</p><p><strong>第二步骤是在Docker存储库中构建并推送Docker镜像。</strong></p><pre><code class="hljs java">docker build:  image: docker:stable  stage: build and push docker image  before_script:    - source .$&#123;CI_COMMIT_REF_NAME&#125;.env  script:    - docker build --build-arg SPRING_ACTIVE_PROFILE=$SPRING_ACTIVE_PROFILE -t $DOCKER_REPO .    - docker login -u $DOCKER_USER -p $DOCKER_PASSWORD docker.io    - docker push $DOCKER_REPO</code></pre><p>这一步骤，我们不得不使用docker:19.03.7-dind服务。如你所见，我们使用的是最新的稳定版本的Docker，我们只是在为适当的环境构建镜像，然后对Dockerhub进行身份验证并推送镜像。</p><p>我们脚本的最后一部分是：</p><pre><code class="hljs bash">deploy:  image: ubuntu:latest  stage: deploy  before_script:    - <span class="hljs-string">'which ssh-agent || ( apt-get update -y &amp;&amp; apt-get install openssh-client -y )'</span>    - <span class="hljs-built_in">eval</span> $(ssh-agent -s)    - <span class="hljs-built_in">echo</span> <span class="hljs-string">"<span class="hljs-variable">$SSH_PRIVATE_KEY</span>"</span> | tr -d <span class="hljs-string">'\r'</span> | ssh-add -    - mkdir -p ~/.ssh    - chmod 700 ~/.ssh    - <span class="hljs-built_in">echo</span> -e <span class="hljs-string">"Host *\n\tStrictHostKeyChecking no\n\n"</span> &gt; ~/.ssh/config    - <span class="hljs-built_in">source</span> .<span class="hljs-variable">$&#123;CI_COMMIT_REF_NAME&#125;</span>.env  script:    - ssh root@<span class="hljs-variable">$SERVER</span> <span class="hljs-string">"docker stop <span class="hljs-variable">$APP_NAME</span>; docker system prune -a -f; docker pull <span class="hljs-variable">$DOCKER_REPO</span>; docker container run -d --name <span class="hljs-variable">$APP_NAME</span> -p <span class="hljs-variable">$PORT</span>:8080 -e SPRING_PROFILES_ACTIVE=<span class="hljs-variable">$SPRING_ACTIVE_PROFILE</span> <span class="hljs-variable">$DOCKER_REPO</span>"</span></code></pre><p>在此步骤中，我们使用Ubuntu Docker镜像，因此我们可以SSH到我们的应用程序服务器并运行一些Docker命令。其中的部分代码 before_script大部分来自<a href="https://docs.gitlab.com/ee/ci/ssh_keys/" target="_blank" rel="noopener">官方文档</a>，但是，当然，我们可以对其进行一些调整以满足我们的需求。为不对私钥进行验证，添加了以下代码行：</p><pre><code class="hljs bash">- <span class="hljs-built_in">echo</span> -e <span class="hljs-string">"Host *\n\tStrictHostKeyChecking no\n\n"</span> &gt; ~/.ssh/config</code></pre><p>你也可以参考<a href="https://docs.gitlab.com/ee/ci/ssh_keys/#verifying-the-ssh-host-keys" target="_blank" rel="noopener">指南</a>验证私钥。 如你在最后阶段的脚本部分中所见，我们正在执行一些Docker命令。</p><ol><li><strong>停止正在运行的Docker容器：</strong>docker stop $APP_NAME。（这就是我们要在.env文件中定义APP_NAME的原因 ）</li><li><strong>删除所有未运行的Docker镜像：</strong> docker system prune -a -f，这实际上不是强制性的，但我想删除服务器上所有未使用的镜像。</li><li><strong>拉取最新版本的Docker镜像</strong>（该镜像是在上一个阶段中构建并推送的）。</li><li>最后，<strong>使用以下命令运行Docker镜像：</strong></li></ol><pre><code class="hljs java">docker container run -d --name $APP_NAME -p $PORT:<span class="hljs-number">8080</span> -e SPRING_PROFILES_ACTIVE=$SPRING_ACTIVE_PROFILE $DOCKER_REPO</code></pre><p>作者：王延飞</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>5步实现规模化的Kubernetes CI/CD 流水线</title>
    <link href="/2020/05/29/5%E6%AD%A5%E5%AE%9E%E7%8E%B0%E8%A7%84%E6%A8%A1%E5%8C%96%E7%9A%84Kubernetes-CI-CD-%E6%B5%81%E6%B0%B4%E7%BA%BF/"/>
    <url>/2020/05/29/5%E6%AD%A5%E5%AE%9E%E7%8E%B0%E8%A7%84%E6%A8%A1%E5%8C%96%E7%9A%84Kubernetes-CI-CD-%E6%B5%81%E6%B0%B4%E7%BA%BF/</url>
    
    <content type="html"><![CDATA[<h1 id="一、背景"><a href="#一、背景" class="headerlink" title="一、背景"></a>一、背景</h1><p>在近几年，Kubernetes迅速成为了容器编排的事实上的开源标准。与虚拟机不同，Kubernetes在抽象化基础架构的同时可靠地大规模编排容器，这可以帮助开发人员将工作负载与基础架构的复杂性分开。Kubernetes是CI/CD自动化的理想选择，因为它提供了许多内置功能，这些功能使应用程序部署实现标准化和可重用，提高了开发人员的生产力，并加快了云原生应用程序的采用。</p><p>Platform9是成立于2013年的云服务提供商，能够提供业界唯一由SaaS管理的混合云解决方案，使用户能够快速采用云技术并在私有部署或公共云中的任何基础架构上一致地管理VM、Kubernetes和无服务器功能。Platform9提供的Kubernetes解决方案也是业界的佼佼者。</p><p>在2019年巴塞罗那举行的KubeCon + CloudNativeCon大会上，Platform9与1000多名与会者进行了互动，有近500名与会者参与了Kubernetes的相关调查。下图列出了关于最常见的Kubernetes应用场景的调查结果：</p><p><img src="https://www.kubernetes.org.cn/img/2019/12/1-1.png" srcset="/img/loading.gif" alt="img"></p><p>其中，基于Kubernetes的CI/CD自动化是最为广泛的用例，专注于帮助开发人员更快地构建和交付应用程序。但是，在具有全自动和可重复的CI/CD流水线的生产环境中运行Kubernetes（k8s）以及持续的安全检查会带来新的挑战，包括集成的复杂性，持续的产品更新/回滚、自运维、生命周期管理等等。</p><p>解决这些挑战并没有想象中的困难，本文将介绍Platform9如何利用JFrog的产品在Kubernetes上快速实现CI/CD自动化并将其推广到整个组织的方法。</p><h1 id=""><a href="#" class="headerlink" title=""></a></h1><h1 id="二、使用Artifactory和Helm的5步Kubernetes-CI-CD流水线"><a href="#二、使用Artifactory和Helm的5步Kubernetes-CI-CD流水线" class="headerlink" title="二、使用Artifactory和Helm的5步Kubernetes CI / CD流水线"></a>二、使用Artifactory和Helm的5步Kubernetes CI / CD流水线</h1><p>在Platform9提供的方案中，JFrog Artifactory作为微服务的Docker注册表（或多个注册表），是构建CI/CD流水线不可或缺的一部分。同时，使用Artifactory作为Helm Chart存储库，进一步使该制品管理仓库能够提供将容器部署到k8s集群所需的所有集成资源。</p><p>Artifactory还可以作为远程依赖的代理仓库，例如npm、Maven、Gradle和Go等，实现安全的Kubernetes注册表，使您能够跟踪系统中端到端的内容、依赖关系，以及与其他Docker镜像的关系。</p><p><img src="https://www.kubernetes.org.cn/img/2019/12/2-2.png" srcset="/img/loading.gif" alt="img"></p><p><strong>5步实现Kubernetes CI/CD流水线：</strong></p><ul><li><strong>步骤1:</strong> 使用Artifactory中代理的注册表来开发微服务。生成的应用程序包可以是.war或.jar文件；</li><li><strong>步骤2:</strong> 在Ubuntu上使用Tomcat和Java-8创建Docker框架作为基础镜像。将此镜像推送到Artifactory中的Docker注册表中，JFrog Xray也会对其进行扫描，以确保安全性和许可证合规性。</li><li><strong>步骤3:</strong> 通过将.war或.jar文件添加到Docker框架中，为微服务创建Docker镜像，将该镜像推送到Artifactory中的Docker注册表中，并通过Xray对其进行扫描。</li><li><strong>步骤4:</strong> 为微服务创建Helm Chart，并将其推送到Artifactory中的Helm存储库。</li><li><strong>步骤5:</strong> 使用Helm Chart将微服务从安全的Docker注册表部署到Kubernetes集群。</li></ul><p>在上述流水线当中，除了负责制品仓库管理，Artifactory还可以提供质量的可审核性，因为它捕获了整个CI/CD流程中产生的所有大量有价值的元数据，包含：</p><ul><li>构建和环境信息</li><li>模块的依赖关系</li><li>CI服务器，如Jenkins，提供的相关信息</li><li>发布模块及其依赖的详细许可证分析</li><li>发布历史信息等</li></ul><p>使用Artifactory，可以跟踪负责生成应用程序层（例如WAR文件）的CI任务，该应用程序层是Docker镜像层的一部分。通过比较两次构建，可以显示构建之间的差异，从而轻松跟踪哪个构建发布到了Docker镜像的哪个层，近而继续跟踪到代码的提交。</p><h1 id="三、流水线特性解析"><a href="#三、流水线特性解析" class="headerlink" title="三、流水线特性解析"></a>三、流水线特性解析</h1><p><strong>3.1 JFrog Artifactory和Xray确保软件交付的自动化</strong></p><p>Artifactory是一个通用的制品仓库管理平台，无论组织中的微服务在哪里运行，它都可以满足所有CI/CD的需求。Artifactory通过提供完全的Docker兼容性，使开发人员能够将容器化的微服务部署到Kubernetes集群中。将应用程序包推送到Artifactory仓库后，您可以在开发、测试和发布阶段继续验证和升级您的容器，最后将其部署到Kubernetes中的生产集群。如之前的分析，Artifactory还为所有应用程序包提供了完整的可审核性和可追溯性。</p><p>JFrog Xray对Docker镜像执行深度递归扫描，并识别所有层和依赖项中的安全漏洞。它还会检查以确保所有软件组件的许可证均符合组织的策略。这有助于阻止易受攻击且不合规的软件投入生产。而且，Xray提供的持续扫描能力，可以确保发现新问题或更改策略时的持续安全性。</p><p><strong>3.2 Helm Charts使CI/CD工具部署自动化</strong></p><p>一旦您的Kubernetes基础架构启动并运行，Kubernetes Helm便使您能够通过轻松安装、更新和删除来快速可靠地配置容器应用程序。它为开发人员提供了一种打包应用程序并与Kubernetes社区共享的工具。它使软件供应商只需按一下按钮即可提供其容器化的应用程序。通过一个命令或单击几下鼠标，用户就可以为开发测试或生产环境安装Kubernetes应用程序。大多数流行的CI/CD工具集都可以通过Helm chart获得。</p><p>Platform9应用目录为Helm图表提供了易于访问的按钮式部署。您可以使用App Catalog UI来选择和部署您喜欢的CI/CD流水线工具（例如，Jenkins、Spinnaker、Artifactory、Xray等）。您可以搜索，一键部署或配置它。</p><p><img src="https://www.kubernetes.org.cn/img/2019/12/3-1.png" srcset="/img/loading.gif" alt="img"></p><h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h1 id="四、Platform9的托管Kubernetes服务消除了操作复杂性"><a href="#四、Platform9的托管Kubernetes服务消除了操作复杂性" class="headerlink" title="四、Platform9的托管Kubernetes服务消除了操作复杂性"></a>四、Platform9的托管Kubernetes服务消除了操作复杂性</h1><p>如果您要解决的主要业务问题，与提高开发人员的生产力，将软件更快地推向市场，以及在生产环境中运行可靠的应用程序有关，那么您是否值得花时间处理运维Kubernetes的麻烦和复杂性？您是否拥有人员和技能来工作和运维大型Kubernetes集群？</p><p>运维您自己的大规模Kubernetes基础架构令人生畏。错误选择的后果是持久的，并且会影响应用程序的可用性、性能和敏捷性。虽然在短期内内部构建解决方案可能会更便宜，但您的设计质量可能较低，或者存在一些缺陷。这些缺陷在您投入生产后才会意识到，从而最终会花费更多的钱。</p><p>虽然安装和管理Kubernetes不会推动业务向前发展，但快速为客户部署新的应用程序和版本却可以。对于开发人员，Kubernetes的最终用户，平台可用性是关键。他们不在乎是谁建造它或如何建造它的：他们只是想动手实践它，并使其运转良好。</p><p>尽管开发人员不关心集群的实现细节或其运维状态，但是仍然需要有人进行运维工作以确保集群是最新、健康且安全的。Platform9管理您的Kubernetes环境，为您提供全自动运维，并在裸机、VMware、公共云或边缘节点上实现99.9％的SLA。这使您的DevOps团队可以腾出时间专注于重要的事情：通过更快地构建更好的应用程序来响应客户需求。</p><p><img src="https://www.kubernetes.org.cn/img/2019/12/4-2.png" srcset="/img/loading.gif" alt="img"></p><h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h1 id="五、总结"><a href="#五、总结" class="headerlink" title="五、总结"></a>五、总结</h1><p>通过上述Platform9推荐的解决方案来看，基于JFrog的Artifactory和Xray，结合Helm Chart，能够方便、快捷、清晰地搭建适用于规模化Kubernetes集群的CI/CD自动化流水线。该方案在保证应用构建和交付的快速、安全、可重复的同时，还能为所有交付的应用提供完整的可审核性和可追溯性。</p><p>作者： JFrogchina</p>]]></content>
    
    
    
    <tags>
      
      <tag>K8s, CI, CD, Platform9, Artifactory, Helm</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>开发者如何快速搭建本地 Kubernetes 集群？Minikube趟坑记录</title>
    <link href="/2020/05/29/%E5%BC%80%E5%8F%91%E8%80%85%E5%A6%82%E4%BD%95%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BA%E6%9C%AC%E5%9C%B0-Kubernetes-%E9%9B%86%E7%BE%A4%EF%BC%9FMinikube%E8%B6%9F%E5%9D%91%E8%AE%B0%E5%BD%95/"/>
    <url>/2020/05/29/%E5%BC%80%E5%8F%91%E8%80%85%E5%A6%82%E4%BD%95%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BA%E6%9C%AC%E5%9C%B0-Kubernetes-%E9%9B%86%E7%BE%A4%EF%BC%9FMinikube%E8%B6%9F%E5%9D%91%E8%AE%B0%E5%BD%95/</url>
    
    <content type="html"><![CDATA[<h1 id="一、背景"><a href="#一、背景" class="headerlink" title="一、背景"></a><strong>一、背景</strong></h1><p>为啥要在本地搭建 Kubernetes 集群？因为开发者可以在本地快速验证自己实现的功能，接口。众所周知，由于 Kubernetes 部署较为复杂，使得广大开发者和运维人员学习和试用 Kubernetes 的门槛很高，光是部署一套 Kubernetes 集群，就需要部署大量的组件，花费精力较大。为了降低用户体验 Kubernetes 的门槛，Minikube 项目应运而生，它是 Github 上的一个开源项目，提供了一键安装的 Kubernetes 本地集群，支持 MacOS，Linux，Windows。</p><p><strong>谁需要 Minikube</strong> <strong>？</strong></p><ul><li><strong>本地开发 Kubernetes</strong> <strong>应用</strong></li><li><strong>离线开发 Kubernetes</strong> <strong>应用</strong></li><li><strong>体验最新版 Kubernetes</strong></li></ul><p>如果你有以上需求，可以使用 Minikube。 Minikube 大大简化了的开发者部署自己的服务到 Kubernetes，因为这个本地集群可以部署在自己的笔记本，亲测 8C16G笔记本跑起来很轻松，随时可以启停，不依赖网络连接。开发者可以在自己的笔记本里，运行Kubernetes 的 Pods，快速验证自己的服务功能是否生效。</p><p>被纳入到了 Kubernetes 官方项目里-<a href="https://github.com/kubernetes/minikube。" target="_blank" rel="noopener">https://github.com/kubernetes/minikube。</a></p><h1 id="二、搭建趟坑之旅"><a href="#二、搭建趟坑之旅" class="headerlink" title="二、搭建趟坑之旅"></a><strong>二、搭建趟坑之旅</strong></h1><ul><li>Mac 环境安装：</li></ul><p>使用官方地址进行一键安装：</p><p><img src="https://www.kubernetes.org.cn/img/2019/12/1-4.png" srcset="/img/loading.gif" alt="img"></p><ul><li><strong>坑点</strong> <strong>：二进制包下载需翻墙</strong></li></ul><p>官方文档给的下载地址需要访问谷歌的服务器storage.googleapis.com，有时候下载容易失败，可以切换到阿里云的下载地址：</p><p><img src="https://www.kubernetes.org.cn/img/2019/12/2-5.png" srcset="/img/loading.gif" alt="img"></p><p>该地址亲测可用。</p><ul><li><strong>启动 Minikube：</strong></li></ul><p>使用官网文档启动：Minikube Start,集群可以正常启动。</p><ul><li><strong>坑点</strong> <strong>：镜像源指向了io</strong></li></ul><p>启动Minikube 之后，运行 pod 一般都会失败，因为 Minikube 默认将它的镜像中心默认指向了 gcr.io,从这里下载过镜像的人都被它深深伤害过，可以在启动参数里指定镜像源和私有镜像库。</p><ul><li><strong>配置私有镜像仓库：</strong></li></ul><p>根据官方文档，在启动时加入参数：” –insecure-registry”</p><p>minikube start –insecure-registry “docker-release-local.demo.jfrog.com” –registry-mirror=<a href="https://registry.docker-cn.com" target="_blank" rel="noopener">https://registry.docker-cn.com</a></p><p>这里–insecure-registry配置的是JFrog 的本地私有 Artifactory Docker 镜像仓库，一般企业内部都有私有镜像库。</p><ul><li><strong>坑点 ：指定私有镜像库不生效</strong></li></ul><p>笔者使用的Minikube v1.2.0 Mac 版本在启动时–insecure-registry并不生效，可以找到主机上 minikube 配置文件目录下的文件进行修改。打开文件：~/.minikube/machines/minikube/config.json，增加记录” docker-release-local.demo.jfrog.com”</p><p><img src="https://www.kubernetes.org.cn/img/2019/12/3-3.png" srcset="/img/loading.gif" alt="img"></p><p>配置完之后镜像拉取默认会从私有镜像仓库寻找镜像。</p><ul><li><strong>从私有镜像仓库拉取镜像</strong></li></ul><p>启动 Minikube 后，在 Kubernetes 集群里创建镜像中心的密钥“regcred”：</p><p>kubectl create secret docker-registry regcred –docker-server=docker-release-local.demo.jfrog.com –docker-username=admin –docker-password=*** –docker-email=wq@jfrogchina.com</p><p>在微服务的 Deployment yaml 文件里使用这个密钥即可让 Pod 通过密钥登录私有镜像仓库，拉取镜像。</p><p><img src="https://www.kubernetes.org.cn/img/2019/12/4-4.png" srcset="/img/loading.gif" alt="img"></p><p>配置好私有镜像仓库之后，你的Pod 镜像拉取速度是秒级的。</p><ul><li><strong>启动应用-Jenkins</strong></li></ul><p>写一个 Jenkins 的部署 yaml 文件，让它运行在 Kubernetes 集群里。</p><p><img src="https://www.kubernetes.org.cn/img/2019/12/5-2.png" srcset="/img/loading.gif" alt="img"></p><p>Jenkins 可以正常运行。</p><p><img src="https://www.kubernetes.org.cn/img/2019/12/6-3.png" srcset="/img/loading.gif" alt="img"></p><ul><li><strong>坑点：Pod 重启时候，Jenkins 数据没了？</strong></li></ul><p>很多应用的配置文件是需要放到 Pod 之外进行管理的，比如 Jenkins。这就需要 Minikube 支持挂载目录。Minikube 官方提供了对挂载目录的支持，默认/data 目录是在重启 Minikube 之后，文件也会保留的目录，可以在/data 目录下创建Jenkins_home目录，然后在Kubernetes 里声明 PV，类型为 hostPath，挂载这个目录。</p><p><img src="https://www.kubernetes.org.cn/img/2019/12/7-2.png" srcset="/img/loading.gif" alt="img"></p><p>声明好这个持久化卷之后，就可以在 Jenkins 的部署 yaml 文件里通过 PVC 来使用这个目录。</p><p><img src="https://www.kubernetes.org.cn/img/2019/12/8-1.png" srcset="/img/loading.gif" alt="img"></p><p>上图可以看到我们把 Jenkins pod 里的/var/Jenkins_home 目录映射到了 Pod 外部，也就是 Minikube 主机上的/data/Jenkins-home,这样即使 Pod 重启，Minikube 重启也不会丢失数据。</p><ul><li><strong>坑点：挂载目录写失败</strong></li></ul><p>当挂创建好/data/Jenkins-home目录之后，默认只有 root 用户有写权限，Jenkins Pod 启动起来之后，会因为无法写入配置文件而启动失败，此时需要将/data/Jenkins-home的权限赋予 docker 用户，再次启动 pod 即可正常运行。</p><p><img src="https://www.kubernetes.org.cn/img/2019/12/9-1.png" srcset="/img/loading.gif" alt="img"></p><ul><li><strong>访问服务</strong></li></ul><p>当 Jenkins Pod 运行起来之后，可以通过 Jenkins service 的 nodePort 暴露的 31081进行访问，这里注意 ip 是 Minikube 的 ip 地址:</p><p><img src="https://www.kubernetes.org.cn/img/2019/12/10-1.png" srcset="/img/loading.gif" alt="img"></p><h1 id="三、小结"><a href="#三、小结" class="headerlink" title="三、小结"></a><strong>三、小结</strong></h1><p><strong>通过 Minikube</strong> <strong>能够实现开发者微服务的本地快速部署，对自己开发的功能，接口进行本地的快速验证，大大提升开发者的交付质量和效率。</strong></p><h1 id="四、附录"><a href="#四、附录" class="headerlink" title="四、附录"></a><strong>四、附录</strong></h1><p><strong>文中用到的代码仓库在这里，欢迎想动手实践的同学进行 clone</strong> <strong>：</strong><a href="https://github.com/alexwang66/sample-microservices-k8s" target="_blank" rel="noopener">https://github.com/alexwang66/sample-microservices-k8s</a></p><p>作者： JFrogchina</p>]]></content>
    
    
    
    <tags>
      
      <tag>Cloudnative, K8s, Minikube，Jenkins</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>使用DevOps从优秀到卓越</title>
    <link href="/2020/05/29/%E4%BD%BF%E7%94%A8DevOps%E4%BB%8E%E4%BC%98%E7%A7%80%E5%88%B0%E5%8D%93%E8%B6%8A/"/>
    <url>/2020/05/29/%E4%BD%BF%E7%94%A8DevOps%E4%BB%8E%E4%BC%98%E7%A7%80%E5%88%B0%E5%8D%93%E8%B6%8A/</url>
    
    <content type="html"><![CDATA[<p>DevOps一直不断发展。自从2009年有了这个此概念以来，DevOps的发展状态就以每年指数级的速度增长。在2019年飞速发展的过程中，各种规模的组织（从企业到初创企业）都对DevOps充满热情。每个组织都有自己的DevOps实践经历。其中一些DevOps实践经历尚未开始，一些实践还在起步阶段，有些实践已经成熟，有些实践已经达到极致。与其他实践不同，DevOps实践永无止境，因为它涉及持续改进。</p><p>随着企业逐渐数字化和软件驱动，人们对DevOps本质和发展潜力有了更大的认识。不仅工程师、技术领导者，还有商业领导者都对DevOps的概念，实践和应用感兴趣。人们对越来越需要DevOps取得商业效益。</p><p>《<a href="https://services.google.com/fh/files/misc/state-of-devops-2019.pdf" target="_blank" rel="noopener">DevOps2019年状况报告》</a>，可以了解DevOps如何塑造跨行业的软件交付的。这份报告总结了软件交付的趋势和挑战。它可以帮助团队可用于改善软件交付性能，并最终实现卓越性能。</p><p>在本报告中，IT性能称为软件交付性能，以区分软件交付工作与IT服务台和其他支持功能。这是一个期待已久、受欢迎的变化。我还喜欢的一项重要更改，是它增加了完成软件交付周期的操作指标。该报告重点介绍了关于“ <strong>软件交付和</strong> <strong>运维</strong> <strong>（SDO）性能</strong> 指标 ”的5项指标，它们侧重于系统级效果。这有助于避免软件度量标准的常见陷阱，后者常常使不同的功能相互冲突，并导致以总体效果为代价的局部优化。</p><p><img src="https://www.kubernetes.org.cn/img/2020/02/%E5%9B%BE%E7%89%871-1.png" srcset="/img/loading.gif" alt="img"></p><center>图1： 5个SDO性能指标</center><p>该报告重点介绍了软件交付性能的四个方面，如下所示：</p><ol><li><strong><em>部署频率</em></strong>–对于你从事的主要应用程序或服务，你的组织多久部署一次代码？</li><li><strong><em>服务***</em></strong>变更的<strong>**</strong>交付<strong>**</strong>时间***–对于你正在工作的主应用程序或服务，你的变更的交付时间是多少（即，从代码提交到成功在生产中运行的代码需要多长时间）？</li><li><strong><em>恢复服务的时间</em></strong>–对于你正在使用的主应用程序或服务，发生服务故障（例如，计划外的服务中断，服务障碍）时，恢复服务通常需要多长时间？</li><li><strong><em>服务修改的***</em></strong>失败率***–对于你使用的主应用程序或服务，多少百分比的修改，会导致服务质量下降或需要立即补救（例如服务故障或服务中断，需要热修复，回滚，打补丁）？</li></ol><p>对如上四个方面的衡量，会有四个性能：卓越，高，中和低。下表（引用上文的报告）说明了各个方面的详细信息。</p><p><img src="https://www.kubernetes.org.cn/img/2020/02/%E5%9B%BE%E7%89%872.jpg" srcset="/img/loading.gif" alt="img"></p><center>图2：软件交付性能的各个方面</center><p>另一个方面，我强烈建议添加到此列表中的是“ <strong><em>团队***</em></strong>参与<strong>**</strong>度指数*** ”，即团队的快乐程度和参与度。我认为团队性能与团队敬业度成正比。团队参与度越高，即团队越快乐和参与度越高，他们产生的结果就越好。</p><p>报告中的另一个主题是“ <strong>J曲线转换</strong>”。下图突出显示了，自动化如何帮助低性能升到中等性能水平，以及测试要求，技术债务和复杂性的增加变得手动控制，从而导致进度变慢。这是一个有趣且“ 值得注意 ”的观察。它强调了自动化并非总是最好的办法。如果你使错误的流程自动化，那么你得到的只是错误的结果，而且更快。</p><p><img src="https://www.kubernetes.org.cn/img/2020/02/%E5%9B%BE%E7%89%873.png" srcset="/img/loading.gif" alt="img"></p><center>图3：转换的J曲线</center><p>持续改进，学习和共享以及利用专业知识，可以使你达到高性能或卓越性能水平- 将团队提升为卓越性能的过程需要的不仅仅是工具。在各个级别（即团队级别，领导级别和赞助者级别）的坚持和毅力是至关重要，会帮助你从中低性能级别突破，以发挥团队最大潜力。 如果我们开始走上实现卓越性能的道路，你会发现自动化，技术实践和持续改进计划是你旅途的催化剂。而测试需求，技术债务和增加的复杂性将成为你的阻碍。我发现锚定和引擎（SailBoat Retrospective）格式提供了一种快速有趣的方法，帮助我们在一幅图片中形象化看到催化剂(engines) 和阻滞剂 (anchors)的关系 （如下所示）。</p><p><img src="https://www.kubernetes.org.cn/img/2020/02/%E5%9B%BE%E7%89%875.png" srcset="/img/loading.gif" alt="img"></p><center>图4：卓越性能之旅</center><h2 id="行业看到了更多的卓越性能"><a href="#行业看到了更多的卓越性能" class="headerlink" title="行业看到了更多的卓越性能"></a>行业看到了更多的卓越性能</h2><p>报告证实，卓越性能的比例几乎增加到原来的三倍，低性能的比例下降了，中等性能的比例上升了。要注意的一项主要观察结果是，从低性能到中性能再到高性能的移动不是一个方向。当面对复杂性增加时，团队（正如J曲线中突出显示）可以从高位降为中级，也可以从中级降为低级。总体而言，很高兴看到向上增长的趋势。</p><p><img src="https://www.kubernetes.org.cn/img/2020/02/%E5%9B%BE%E7%89%876.jpg" srcset="/img/loading.gif" alt="img"></p><center>图5：不同性能集群</center><h1 id="未来的方向"><a href="#未来的方向" class="headerlink" title="未来的方向"></a>未来的方向</h1><p>软件交付性能可以通过多种方式决定业务成果。组织推动软件交付性能的能力包括文化，技术实践，清晰的变更过程，持续交付以及有价值的成果。这些功能并不是一蹴而就的，需要对组织DNA进行根本性的改变。</p><p>根据我在不同行业和公司中工作的经验，我可以确认这些软件交付性能不是静态的。上面列出的任何功能的变化都会影响软件交付性能，你可能会发现集群性能来回从一个级别波动到另一个级别。关键是保持专注，并通过将其嵌入组织的工作方式，定期来维持它。</p><p>作者：王延飞</p>]]></content>
    
    
    
    <tags>
      
      <tag>DevOps,SDO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>使用Kubeadm在外部OpenStack云厂商部署Kubernetes集群</title>
    <link href="/2020/05/29/%E4%BD%BF%E7%94%A8Kubeadm%E5%9C%A8%E5%A4%96%E9%83%A8OpenStack%E4%BA%91%E5%8E%82%E5%95%86%E9%83%A8%E7%BD%B2Kubernetes%E9%9B%86%E7%BE%A4/"/>
    <url>/2020/05/29/%E4%BD%BF%E7%94%A8Kubeadm%E5%9C%A8%E5%A4%96%E9%83%A8OpenStack%E4%BA%91%E5%8E%82%E5%95%86%E9%83%A8%E7%BD%B2Kubernetes%E9%9B%86%E7%BE%A4/</url>
    
    <content type="html"><![CDATA[<p>本文档描述了如何在CentOS上使用kubeadm安装单个控制面板节点（主节点）的Kubernetes集群（ v1.15 ），然后部署到一个外部OpenStack云厂商，并且使用Cinder CSI插件将Cinder存储卷用作Kubernetes中的持久卷。</p><h3 id="OpenStack的准备工作"><a href="#OpenStack的准备工作" class="headerlink" title="OpenStack的准备工作"></a>OpenStack的准备工作</h3><p>Kubernetes集群要在OpenStack VM上运行，因此让我们首先需要在OpenStack中创建一些东西。</p><ul><li>Kubernetes集群的项目/租户</li><li>Kubernetes集群项目的用户，以查询节点信息和附加存储卷等</li><li>专用网络和子网</li><li>专用网络的路由器，并将其连接到公用网络以获取浮动IP</li><li>所有Kubernetes 虚拟机的安全组</li><li>一个虚拟机作为控制面板节点（主节点），几个虚拟机作为工作节点</li></ul><p>安全组将具有以下规则来打开Kubernetes的端口。</p><h4 id="控制面板节点（主节点）"><a href="#控制面板节点（主节点）" class="headerlink" title="控制面板节点（主节点）"></a>控制面板节点（主节点）</h4><table><thead><tr><th align="center">协议</th><th align="center">端口号</th><th align="center">描述</th></tr></thead><tbody><tr><td align="center">TCP协议</td><td align="center">6443</td><td align="center">Kubernetes API Server</td></tr><tr><td align="center">TCP协议</td><td align="center">2379-2380</td><td align="center">etcd server client API</td></tr><tr><td align="center">TCP协议</td><td align="center">10250</td><td align="center">Kubelet API</td></tr><tr><td align="center">TCP协议</td><td align="center">10251</td><td align="center">kube-scheduler</td></tr><tr><td align="center">TCP协议</td><td align="center">10252</td><td align="center">kube-controller-manager</td></tr><tr><td align="center">TCP协议</td><td align="center">10255</td><td align="center">只读 Kubelet API</td></tr></tbody></table><h4 id="工作节点"><a href="#工作节点" class="headerlink" title="工作节点"></a>工作节点</h4><table><thead><tr><th align="center">协议</th><th align="center">端口号</th><th align="center">描述</th></tr></thead><tbody><tr><td align="center">TCP协议</td><td align="center">10250</td><td align="center">Kubelet API</td></tr><tr><td align="center">TCP协议</td><td align="center">10255</td><td align="center">Read-only Kubelet API</td></tr><tr><td align="center">TCP协议</td><td align="center">30000-32767</td><td align="center">NodePort Services</td></tr></tbody></table><h4 id="控制面板节点（主节点）和工作节点上的CNI端口"><a href="#控制面板节点（主节点）和工作节点上的CNI端口" class="headerlink" title="控制面板节点（主节点）和工作节点上的CNI端口"></a>控制面板节点（主节点）和工作节点上的CNI端口</h4><table><thead><tr><th align="center">协议</th><th align="center">端口号</th><th align="center">描述</th></tr></thead><tbody><tr><td align="center">TCP协议</td><td align="center">179</td><td align="center">Calico BGP network</td></tr><tr><td align="center">TCP协议</td><td align="center">9099</td><td align="center">Calico felix（健康检查）</td></tr><tr><td align="center">UDP协议</td><td align="center">8285</td><td align="center">Flannel</td></tr><tr><td align="center">UDP协议</td><td align="center">8472</td><td align="center">Flannel</td></tr><tr><td align="center">TCP协议</td><td align="center">6781-6784</td><td align="center">Weave Net</td></tr><tr><td align="center">UDP协议</td><td align="center">6783-6784</td><td align="center">Weave Net</td></tr></tbody></table><p>仅在使用特定的CNI插件时才需要打开CNI特定的端口。在本指南中，我们将使用Weave Net。在安全组中仅需要打开Weave Net端口（TCP 6781-6784和UDP 6783-6784）。</p><p>控制面板节点（主节点）至少需要2个内核和4GB RAM。启动虚拟机后，请验证其主机名，并确保其与Nova中的节点名相同。如果主机名不可解析，请将其添加到中/etc/hosts。</p><p>例如，如果虚拟机名为master1，并且它的内部IP是192.168.1.4。将其添加到/etc/hosts并将主机名设置为master1。</p><pre><code class="hljs bash"><span class="hljs-built_in">echo</span> <span class="hljs-string">"192.168.1.4 master1"</span> &gt;&gt; /etc/hostshostnamectl <span class="hljs-built_in">set</span>-hostname master1</code></pre><h3 id="安装Docker和Kubernetes"><a href="#安装Docker和Kubernetes" class="headerlink" title="安装Docker和Kubernetes"></a>安装Docker和Kubernetes</h3><p>接下来，我们将按照官方文档使用kubeadm安装docker和Kubernetes。</p><p>按照<a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes/" target="_blank" rel="noopener">容器运行时</a>文档中的步骤安装Docker 。</p><p>请注意，<a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes/#cgroup-drivers" target="_blank" rel="noopener">最佳做法是将systemd用作Kubernetes 的cgroup驱动程序</a>。如果你使用内部容器注册表，请将其添加到docker 配置中。</p><pre><code class="hljs bash"><span class="hljs-comment"># Install Docker CE</span><span class="hljs-comment">## Set up the repository</span><span class="hljs-comment">### Install required packages.</span>yum install yum-utils device-mapper-persistent-data lvm2<span class="hljs-comment">### Add Docker repository.</span>yum-config-manager \  --add-repo \  https://download.docker.com/linux/centos/docker-ce.repo<span class="hljs-comment">## Install Docker CE.</span>yum update &amp;&amp; yum install docker-ce-18.06.2.ce<span class="hljs-comment">## Create /etc/docker directory.</span>mkdir /etc/docker<span class="hljs-comment"># Configure the Docker daemon</span>cat &gt; /etc/docker/daemon.json &lt;&lt;EOF&#123;  <span class="hljs-string">"exec-opts"</span>: [<span class="hljs-string">"native.cgroupdriver=systemd"</span>],  <span class="hljs-string">"log-driver"</span>: <span class="hljs-string">"json-file"</span>,  <span class="hljs-string">"log-opts"</span>: &#123;    <span class="hljs-string">"max-size"</span>: <span class="hljs-string">"100m"</span>  &#125;,  <span class="hljs-string">"storage-driver"</span>: <span class="hljs-string">"overlay2"</span>,  <span class="hljs-string">"storage-opts"</span>: [    <span class="hljs-string">"overlay2.override_kernel_check=true"</span>  ]&#125;EOFmkdir -p /etc/systemd/system/docker.service.d<span class="hljs-comment"># Restart Docker</span>systemctl daemon-reloadsystemctl restart dockersystemctl <span class="hljs-built_in">enable</span> docker</code></pre><p>按照<a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/" target="_blank" rel="noopener">Kubeadm安装文档</a>中的步骤，安装kubeadm。</p><pre><code class="hljs bash">cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64enabled=1gpgcheck=1repo_gpgcheck=1gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpgEOF<span class="hljs-comment"># Set SELinux in permissive mode (effectively disabling it)</span><span class="hljs-comment"># Caveat: In a production environment you may not want to disable SELinux, please refer to Kubernetes documents about SELinux</span>setenforce 0sed -i <span class="hljs-string">'s/^SELINUX=enforcing$/SELINUX=permissive/'</span> /etc/selinux/configyum install -y kubelet kubeadm kubectl --disableexcludes=kubernetessystemctl <span class="hljs-built_in">enable</span> --now kubeletcat &lt;&lt;EOF &gt;  /etc/sysctl.d/k8s.confnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOFsysctl --system<span class="hljs-comment"># check if br_netfilter module is loaded</span>lsmod | grep br_netfilter<span class="hljs-comment"># if not, load it explicitly with </span>modprobe br_netfilter</code></pre><p>根据官方文档<a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/" target="_blank" rel="noopener">创建单个控制面板（主节点）群集中找到</a>，我们来创建单个控制面板（主节点）集群</p><p>我们将主要遵循该文档来创建集群，但还会为云厂商添加其他内容。为了使事情更清楚，对于控制面板节点（主节点）我们将使用一个kubeadm-config.yml配置文件。在此配置中，我们指定使用外部OpenStack云厂商，以及在何处找到其配置。**我们还在API服务器的运行时配置中启用了存储API，因此我们可以将OpenStack存储卷用作Kubernetes中的持久卷。</p><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">kubeadm.k8s.io/v1beta1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">InitConfiguration</span><span class="hljs-attr">nodeRegistration:</span>  <span class="hljs-attr">kubeletExtraArgs:</span>    <span class="hljs-attr">cloud-provider:</span> <span class="hljs-string">"external"</span><span class="hljs-meta">---</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">kubeadm.k8s.io/v1beta2</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterConfiguration</span><span class="hljs-attr">kubernetesVersion:</span> <span class="hljs-string">"v1.15.1"</span><span class="hljs-attr">apiServer:</span>  <span class="hljs-attr">extraArgs:</span>    <span class="hljs-attr">enable-admission-plugins:</span> <span class="hljs-string">NodeRestriction</span>    <span class="hljs-attr">runtime-config:</span> <span class="hljs-string">"storage.k8s.io/v1=true"</span><span class="hljs-attr">controllerManager:</span>  <span class="hljs-attr">extraArgs:</span>    <span class="hljs-attr">external-cloud-volume-plugin:</span> <span class="hljs-string">openstack</span>  <span class="hljs-attr">extraVolumes:</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">"cloud-config"</span>    <span class="hljs-attr">hostPath:</span> <span class="hljs-string">"/etc/kubernetes/cloud-config"</span>    <span class="hljs-attr">mountPath:</span> <span class="hljs-string">"/etc/kubernetes/cloud-config"</span>    <span class="hljs-attr">readOnly:</span> <span class="hljs-literal">true</span>    <span class="hljs-attr">pathType:</span> <span class="hljs-string">File</span><span class="hljs-attr">networking:</span>  <span class="hljs-attr">serviceSubnet:</span> <span class="hljs-string">"10.96.0.0/12"</span>  <span class="hljs-attr">podSubnet:</span> <span class="hljs-string">"10.224.0.0/16"</span>  <span class="hljs-attr">dnsDomain:</span> <span class="hljs-string">"cluster.local"</span></code></pre><p>现在，我们将为OpenStack 创建配置/etc/kubernetes/cloud-config。请注意，此处的租户是我们一开始为所有Kubernetes VM创建的租户。所有虚拟机都应在该项目/租户中启动。另外，你需要在此租户中创建一个用户，以便Kubernetes进行查询。 ca-file 是OpenStack API端点的CA根证书，目前，云厂商不允许不安全的连接（跳过CA检查），例如<a href="https://openstack.cloud:5000/v3" target="_blank" rel="noopener">https://openstack.cloud:5000/v3</a> 。</p><pre><code class="hljs yaml"><span class="hljs-string">[Global]</span><span class="hljs-string">region=RegionOne</span><span class="hljs-string">username=username</span><span class="hljs-string">password=password</span><span class="hljs-string">auth-url=https://openstack.cloud:5000/v3</span><span class="hljs-string">tenant-id=14ba698c0aec4fd6b7dc8c310f664009</span><span class="hljs-string">domain-id=default</span><span class="hljs-string">ca-file=/etc/kubernetes/ca.pem</span><span class="hljs-string">[LoadBalancer]</span><span class="hljs-string">subnet-id=b4a9a292-ea48-4125-9fb2-8be2628cb7a1</span><span class="hljs-string">floating-network-id=bc8a590a-5d65-4525-98f3-f7ef29c727d5</span><span class="hljs-string">[BlockStorage]</span><span class="hljs-string">bs-version=v2</span><span class="hljs-string">[Networking]</span><span class="hljs-string">public-network-name=public</span><span class="hljs-string">ipv6-support-disabled=false</span></code></pre><p>接下来，我们来运行kubeadm以启动控制面板节点（主节点）</p><pre><code class="hljs bash">kubeadm init --config=kubeadm-config.yml</code></pre><p>完成初始化后，将admin config复制到.kube路径下</p><pre><code class="hljs bash">mkdir -p <span class="hljs-variable">$HOME</span>/.kubesudo cp -i /etc/kubernetes/admin.conf <span class="hljs-variable">$HOME</span>/.kube/configsudo chown $(id -u):$(id -g) <span class="hljs-variable">$HOME</span>/.kube/config</code></pre><p>在此阶段，控制面板节点（主节点）已创建但尚未就绪。所有节点都有污点node.cloudprovider.kubernetes.io/uninitialized=true:NoSchedule，正在等待由 cloud-controller-manager（云厂商控制器管理器）初始化。</p><pre><code class="hljs bash"><span class="hljs-comment"># kubectl describe no master1</span>Name:               master1Roles:              master......Taints:             node-role.kubernetes.io/master:NoSchedule                    node.cloudprovider.kubernetes.io/uninitialized=<span class="hljs-literal">true</span>:NoSchedule                    node.kubernetes.io/not-ready:NoSchedule......</code></pre><p>现在，根据<a href="https://github.com/kubernetes/cloud-provider-openstack/blob/master/docs/using-controller-manager-with-kubeadm.md" target="_blank" rel="noopener">将控制器管理器与kubeadm结合使用</a>文档，将OpenStack云控制器管理器部署到kubernetes集群中。</p><p>使用cloud-config为openstack云厂商创建一个密钥。</p><pre><code class="hljs yaml"><span class="hljs-string">kubectl</span> <span class="hljs-string">create</span> <span class="hljs-string">secret</span> <span class="hljs-string">-n</span> <span class="hljs-string">kube-system</span> <span class="hljs-string">generic</span> <span class="hljs-string">cloud-config</span> <span class="hljs-string">--from-literal=cloud.conf="$(cat</span> <span class="hljs-string">/etc/kubernetes/cloud-config)"</span> <span class="hljs-string">--dry-run</span> <span class="hljs-string">-o</span> <span class="hljs-string">yaml</span> <span class="hljs-string">&gt;</span> <span class="hljs-string">cloud-config-secret.yaml</span><span class="hljs-string">kubectl</span> <span class="hljs-string">apply</span> <span class="hljs-string">-f</span> <span class="hljs-string">cloud-config-secret.yaml</span></code></pre><p>获取OpenStack API端点的CA证书，并将其放入/etc/kubernetes/ca.pem中。</p><p>创建RBAC资源。</p><pre><code class="hljs bash">kubectl apply -f https://github.com/kubernetes/cloud-provider-openstack/raw/release-1.15/cluster/addons/rbac/cloud-controller-manager-roles.yamlkubectl apply -f https://github.com/kubernetes/cloud-provider-openstack/raw/release-1.15/cluster/addons/rbac/cloud-controller-manager-role-bindings.yaml</code></pre><p>我们将以DaemonSet而不是Pod的形式运行OpenStack云控制器管理器。该管理器仅在控制面板节点（主节点）上运行，因此，如果有多个控制面板节点（主节点），则将运行多个Pod以实现高可用性。创建如下的openstack-cloud-controller-manager-ds.yaml文件，然后应用它。</p><pre><code class="hljs yaml"><span class="hljs-meta">---</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ServiceAccount</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">cloud-controller-manager</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span><span class="hljs-meta">---</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">apps/v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">DaemonSet</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">openstack-cloud-controller-manager</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">k8s-app:</span> <span class="hljs-string">openstack-cloud-controller-manager</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">selector:</span>    <span class="hljs-attr">matchLabels:</span>      <span class="hljs-attr">k8s-app:</span> <span class="hljs-string">openstack-cloud-controller-manager</span>  <span class="hljs-attr">updateStrategy:</span>    <span class="hljs-attr">type:</span> <span class="hljs-string">RollingUpdate</span>  <span class="hljs-attr">template:</span>    <span class="hljs-attr">metadata:</span>      <span class="hljs-attr">labels:</span>        <span class="hljs-attr">k8s-app:</span> <span class="hljs-string">openstack-cloud-controller-manager</span>    <span class="hljs-attr">spec:</span>      <span class="hljs-attr">nodeSelector:</span>        <span class="hljs-attr">node-role.kubernetes.io/master:</span> <span class="hljs-string">""</span>      <span class="hljs-attr">securityContext:</span>        <span class="hljs-attr">runAsUser:</span> <span class="hljs-number">1001</span>      <span class="hljs-attr">tolerations:</span>      <span class="hljs-bullet">-</span> <span class="hljs-attr">key:</span> <span class="hljs-string">node.cloudprovider.kubernetes.io/uninitialized</span>        <span class="hljs-attr">value:</span> <span class="hljs-string">"true"</span>        <span class="hljs-attr">effect:</span> <span class="hljs-string">NoSchedule</span>      <span class="hljs-bullet">-</span> <span class="hljs-attr">key:</span> <span class="hljs-string">node-role.kubernetes.io/master</span>        <span class="hljs-attr">effect:</span> <span class="hljs-string">NoSchedule</span>      <span class="hljs-bullet">-</span> <span class="hljs-attr">effect:</span> <span class="hljs-string">NoSchedule</span>        <span class="hljs-attr">key:</span> <span class="hljs-string">node.kubernetes.io/not-ready</span>      <span class="hljs-attr">serviceAccountName:</span> <span class="hljs-string">cloud-controller-manager</span>      <span class="hljs-attr">containers:</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">openstack-cloud-controller-manager</span>          <span class="hljs-attr">image:</span> <span class="hljs-string">docker.io/k8scloudprovider/openstack-cloud-controller-manager:v1.15.0</span>          <span class="hljs-attr">args:</span>            <span class="hljs-bullet">-</span> <span class="hljs-string">/bin/openstack-cloud-controller-manager</span>            <span class="hljs-bullet">-</span> <span class="hljs-string">--v=1</span>            <span class="hljs-bullet">-</span> <span class="hljs-string">--cloud-config=$(CLOUD_CONFIG)</span>            <span class="hljs-bullet">-</span> <span class="hljs-string">--cloud-provider=openstack</span>            <span class="hljs-bullet">-</span> <span class="hljs-string">--use-service-account-credentials=true</span>            <span class="hljs-bullet">-</span> <span class="hljs-string">--address=127.0.0.1</span>          <span class="hljs-attr">volumeMounts:</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/etc/kubernetes/pki</span>              <span class="hljs-attr">name:</span> <span class="hljs-string">k8s-certs</span>              <span class="hljs-attr">readOnly:</span> <span class="hljs-literal">true</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/etc/ssl/certs</span>              <span class="hljs-attr">name:</span> <span class="hljs-string">ca-certs</span>              <span class="hljs-attr">readOnly:</span> <span class="hljs-literal">true</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/etc/config</span>              <span class="hljs-attr">name:</span> <span class="hljs-string">cloud-config-volume</span>              <span class="hljs-attr">readOnly:</span> <span class="hljs-literal">true</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/usr/libexec/kubernetes/kubelet-plugins/volume/exec</span>              <span class="hljs-attr">name:</span> <span class="hljs-string">flexvolume-dir</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/etc/kubernetes</span>              <span class="hljs-attr">name:</span> <span class="hljs-string">ca-cert</span>              <span class="hljs-attr">readOnly:</span> <span class="hljs-literal">true</span>          <span class="hljs-attr">resources:</span>            <span class="hljs-attr">requests:</span>              <span class="hljs-attr">cpu:</span> <span class="hljs-string">200m</span>          <span class="hljs-attr">env:</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">CLOUD_CONFIG</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">/etc/config/cloud.conf</span>      <span class="hljs-attr">hostNetwork:</span> <span class="hljs-literal">true</span>      <span class="hljs-attr">volumes:</span>      <span class="hljs-bullet">-</span> <span class="hljs-attr">hostPath:</span>          <span class="hljs-attr">path:</span> <span class="hljs-string">/usr/libexec/kubernetes/kubelet-plugins/volume/exec</span>          <span class="hljs-attr">type:</span> <span class="hljs-string">DirectoryOrCreate</span>        <span class="hljs-attr">name:</span> <span class="hljs-string">flexvolume-dir</span>      <span class="hljs-bullet">-</span> <span class="hljs-attr">hostPath:</span>          <span class="hljs-attr">path:</span> <span class="hljs-string">/etc/kubernetes/pki</span>          <span class="hljs-attr">type:</span> <span class="hljs-string">DirectoryOrCreate</span>        <span class="hljs-attr">name:</span> <span class="hljs-string">k8s-certs</span>      <span class="hljs-bullet">-</span> <span class="hljs-attr">hostPath:</span>          <span class="hljs-attr">path:</span> <span class="hljs-string">/etc/ssl/certs</span>          <span class="hljs-attr">type:</span> <span class="hljs-string">DirectoryOrCreate</span>        <span class="hljs-attr">name:</span> <span class="hljs-string">ca-certs</span>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">cloud-config-volume</span>        <span class="hljs-attr">secret:</span>          <span class="hljs-attr">secretName:</span> <span class="hljs-string">cloud-config</span>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">ca-cert</span>        <span class="hljs-attr">secret:</span>          <span class="hljs-attr">secretName:</span> <span class="hljs-string">openstack-ca-cert</span></code></pre><p>controller manager 运行时，它将查询OpenStack以获取有关节点的信息并删除污点。在节点信息中，你将看到OpenStack中VM的UUID。</p><pre><code class="hljs bash"><span class="hljs-comment"># kubectl describe no master1</span>Name:               master1Roles:              master......Taints:             node-role.kubernetes.io/master:NoSchedule                    node.kubernetes.io/not-ready:NoSchedule......sage:docker: network plugin is not ready: cni config uninitialized......PodCIDR:                     10.224.0.0/24ProviderID:                  openstack:///548e3c46-2477-4ce2-968b-3de1314560a5</code></pre><p>现在安装你喜欢的CNI，控制面板节点（主节点）将准备就绪。</p><p>例如，要安装Weave Net，请运行以下命令：</p><pre><code class="hljs bash">kubectl apply -f <span class="hljs-string">"https://cloud.weave.works/k8s/net?k8s-version=<span class="hljs-variable">$(kubectl version | base64 | tr -d '\n')</span>"</span></code></pre><p>接下来，我们将设置工作节点。</p><p>首先，用在控制面板节点（主节点）中相同的方式，安装docker和kubeadm。要将它们加入集群，我们需要从控制面板节点（主节点）安装输出中获得令牌和ca cert 哈希值。如果它已过期或丢失，我们可以使用以下命令重新创建它。</p><pre><code class="hljs bash"><span class="hljs-comment"># check if token is expired</span>kubeadm token list<span class="hljs-comment"># re-create token and show join command</span>kubeadm token create --<span class="hljs-built_in">print</span>-join-command</code></pre><p>使用上述令牌和ca cert哈希值为工作节点创建kubeadm-config.yml配置文件。</p><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">kubeadm.k8s.io/v1beta2</span><span class="hljs-attr">discovery:</span>  <span class="hljs-attr">bootstrapToken:</span>    <span class="hljs-attr">apiServerEndpoint:</span> <span class="hljs-number">192.168</span><span class="hljs-number">.1</span><span class="hljs-number">.7</span><span class="hljs-string">:6443</span>    <span class="hljs-attr">token:</span> <span class="hljs-string">0c0z4p.dnafh6vnmouus569</span>    <span class="hljs-attr">caCertHashes:</span> <span class="hljs-string">["sha256:fcb3e956a6880c05fc9d09714424b827f57a6fdc8afc44497180905946527adf"]</span><span class="hljs-attr">kind:</span> <span class="hljs-string">JoinConfiguration</span><span class="hljs-attr">nodeRegistration:</span>  <span class="hljs-attr">kubeletExtraArgs:</span>    <span class="hljs-attr">cloud-provider:</span> <span class="hljs-string">"external"</span></code></pre><p>apiServerEndpoint是控制面板节点（主节点），令牌和caCertHashes可从kubeadm token create命令的输出中打印的 join 命令中获取。</p><p>运行kubeadm，工作节点将加入集群。</p><pre><code class="hljs bash">kubeadm join  --config kubeadm-config.yml</code></pre><p>在这个阶段，我们将拥有一个在外部OpenStack云厂商运行的Kubernetes集群。厂商会告知Kubernetes，Kubernetes节点与OpenStack VM之间的映射。如果Kubernetes想要将持久卷附加到Pod，则可以从映射中找出Pod在哪个OpenStack VM上运行，并可以将底层OpenStack存储卷相应地附加到VM。</p><h3 id="部署Cinder-CSI"><a href="#部署Cinder-CSI" class="headerlink" title="部署Cinder CSI"></a>部署Cinder CSI</h3><p>与Cinder的集成由外部Cinder CSI插件提供，正如<a href="https://github.com/kubernetes/cloud-provider-openstack/blob/master/docs/using-cinder-csi-plugin.md" target="_blank" rel="noopener">Cinder CSI</a>文档中所述。</p><p>我们将执行以下步骤来安装Cinder CSI插件。</p><p>首先，使用CA证书为OpenStack的API端点创建一个密钥。与我们在上面的云厂商中使用的证书文件相同。</p><pre><code class="hljs bash">kubectl create secret -n kube-system generic openstack-ca-cert --from-literal=ca.pem=<span class="hljs-string">"<span class="hljs-variable">$(cat /etc/kubernetes/ca.pem)</span>"</span> --dry-run -o yaml &gt; openstack-ca-cert.yamlkubectl apply -f openstack-ca-cert.yaml</code></pre><p>然后创建RBAC资源。</p><pre><code class="hljs bash">kubectl apply -f https://raw.githubusercontent.com/kubernetes/cloud-provider-openstack/release-1.15/manifests/cinder-csi-plugin/cinder-csi-controllerplugin-rbac.yamlkubectl apply -f https://github.com/kubernetes/cloud-provider-openstack/raw/release-1.15/manifests/cinder-csi-plugin/cinder-csi-nodeplugin-rbac.yaml</code></pre><p>Cinder CSI插件包括控制器插件和节点插件。控制器与Kubernetes API和Cinder API通信以 create/attach/detach/delete （创建/附加/分离/删除）Cinder卷。节点插件依次在每个工作节点上运行，以将存储设备（附加的卷）绑定到Pod，并在删除过程中取消绑定。创建cinder-csi-controllerplugin.yaml并应用它以创建csi控制器。</p><pre><code class="hljs yaml"><span class="hljs-attr">kind:</span> <span class="hljs-string">Service</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">csi-cinder-controller-service</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">app:</span> <span class="hljs-string">csi-cinder-controllerplugin</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">selector:</span>    <span class="hljs-attr">app:</span> <span class="hljs-string">csi-cinder-controllerplugin</span>  <span class="hljs-attr">ports:</span>    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">dummy</span>      <span class="hljs-attr">port:</span> <span class="hljs-number">12345</span><span class="hljs-meta">---</span><span class="hljs-attr">kind:</span> <span class="hljs-string">StatefulSet</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">apps/v1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">csi-cinder-controllerplugin</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">serviceName:</span> <span class="hljs-string">"csi-cinder-controller-service"</span>  <span class="hljs-attr">replicas:</span> <span class="hljs-number">1</span>  <span class="hljs-attr">selector:</span>    <span class="hljs-attr">matchLabels:</span>      <span class="hljs-attr">app:</span> <span class="hljs-string">csi-cinder-controllerplugin</span>  <span class="hljs-attr">template:</span>    <span class="hljs-attr">metadata:</span>      <span class="hljs-attr">labels:</span>        <span class="hljs-attr">app:</span> <span class="hljs-string">csi-cinder-controllerplugin</span>    <span class="hljs-attr">spec:</span>      <span class="hljs-attr">serviceAccount:</span> <span class="hljs-string">csi-cinder-controller-sa</span>      <span class="hljs-attr">containers:</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">csi-attacher</span>          <span class="hljs-attr">image:</span> <span class="hljs-string">quay.io/k8scsi/csi-attacher:v1.0.1</span>          <span class="hljs-attr">args:</span>            <span class="hljs-bullet">-</span> <span class="hljs-string">"--v=5"</span>            <span class="hljs-bullet">-</span> <span class="hljs-string">"--csi-address=$(ADDRESS)"</span>          <span class="hljs-attr">env:</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">ADDRESS</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">/var/lib/csi/sockets/pluginproxy/csi.sock</span>          <span class="hljs-attr">imagePullPolicy:</span> <span class="hljs-string">"IfNotPresent"</span>          <span class="hljs-attr">volumeMounts:</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">socket-dir</span>              <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/var/lib/csi/sockets/pluginproxy/</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">csi-provisioner</span>          <span class="hljs-attr">image:</span> <span class="hljs-string">quay.io/k8scsi/csi-provisioner:v1.0.1</span>          <span class="hljs-attr">args:</span>            <span class="hljs-bullet">-</span> <span class="hljs-string">"--provisioner=csi-cinderplugin"</span>            <span class="hljs-bullet">-</span> <span class="hljs-string">"--csi-address=$(ADDRESS)"</span>          <span class="hljs-attr">env:</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">ADDRESS</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">/var/lib/csi/sockets/pluginproxy/csi.sock</span>          <span class="hljs-attr">imagePullPolicy:</span> <span class="hljs-string">"IfNotPresent"</span>          <span class="hljs-attr">volumeMounts:</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">socket-dir</span>              <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/var/lib/csi/sockets/pluginproxy/</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">csi-snapshotter</span>          <span class="hljs-attr">image:</span> <span class="hljs-string">quay.io/k8scsi/csi-snapshotter:v1.0.1</span>          <span class="hljs-attr">args:</span>            <span class="hljs-bullet">-</span> <span class="hljs-string">"--connection-timeout=15s"</span>            <span class="hljs-bullet">-</span> <span class="hljs-string">"--csi-address=$(ADDRESS)"</span>          <span class="hljs-attr">env:</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">ADDRESS</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">/var/lib/csi/sockets/pluginproxy/csi.sock</span>          <span class="hljs-attr">imagePullPolicy:</span> <span class="hljs-string">Always</span>          <span class="hljs-attr">volumeMounts:</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/var/lib/csi/sockets/pluginproxy/</span>              <span class="hljs-attr">name:</span> <span class="hljs-string">socket-dir</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">cinder-csi-plugin</span>          <span class="hljs-attr">image:</span> <span class="hljs-string">docker.io/k8scloudprovider/cinder-csi-plugin:v1.15.0</span>          <span class="hljs-attr">args :</span>            <span class="hljs-bullet">-</span> <span class="hljs-string">/bin/cinder-csi-plugin</span>            <span class="hljs-bullet">-</span> <span class="hljs-string">"--v=5"</span>            <span class="hljs-bullet">-</span> <span class="hljs-string">"--nodeid=$(NODE_ID)"</span>            <span class="hljs-bullet">-</span> <span class="hljs-string">"--endpoint=$(CSI_ENDPOINT)"</span>            <span class="hljs-bullet">-</span> <span class="hljs-string">"--cloud-config=$(CLOUD_CONFIG)"</span>            <span class="hljs-bullet">-</span> <span class="hljs-string">"--cluster=$(CLUSTER_NAME)"</span>          <span class="hljs-attr">env:</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">NODE_ID</span>              <span class="hljs-attr">valueFrom:</span>                <span class="hljs-attr">fieldRef:</span>                  <span class="hljs-attr">fieldPath:</span> <span class="hljs-string">spec.nodeName</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">CSI_ENDPOINT</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">unix://csi/csi.sock</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">CLOUD_CONFIG</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">/etc/config/cloud.conf</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">CLUSTER_NAME</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">kubernetes</span>          <span class="hljs-attr">imagePullPolicy:</span> <span class="hljs-string">"IfNotPresent"</span>          <span class="hljs-attr">volumeMounts:</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">socket-dir</span>              <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/csi</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">secret-cinderplugin</span>              <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/etc/config</span>              <span class="hljs-attr">readOnly:</span> <span class="hljs-literal">true</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/etc/kubernetes</span>              <span class="hljs-attr">name:</span> <span class="hljs-string">ca-cert</span>              <span class="hljs-attr">readOnly:</span> <span class="hljs-literal">true</span>      <span class="hljs-attr">volumes:</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">socket-dir</span>          <span class="hljs-attr">hostPath:</span>            <span class="hljs-attr">path:</span> <span class="hljs-string">/var/lib/csi/sockets/pluginproxy/</span>            <span class="hljs-attr">type:</span> <span class="hljs-string">DirectoryOrCreate</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">secret-cinderplugin</span>          <span class="hljs-attr">secret:</span>            <span class="hljs-attr">secretName:</span> <span class="hljs-string">cloud-config</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">ca-cert</span>          <span class="hljs-attr">secret:</span>            <span class="hljs-attr">secretName:</span> <span class="hljs-string">openstack-ca-cert</span></code></pre><p>创建cinder-csi-nodeplugin.yaml并应用它来创建csi节点。</p><pre><code class="hljs yaml"><span class="hljs-attr">kind:</span> <span class="hljs-string">DaemonSet</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">apps/v1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">csi-cinder-nodeplugin</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">selector:</span>    <span class="hljs-attr">matchLabels:</span>      <span class="hljs-attr">app:</span> <span class="hljs-string">csi-cinder-nodeplugin</span>  <span class="hljs-attr">template:</span>    <span class="hljs-attr">metadata:</span>      <span class="hljs-attr">labels:</span>        <span class="hljs-attr">app:</span> <span class="hljs-string">csi-cinder-nodeplugin</span>    <span class="hljs-attr">spec:</span>      <span class="hljs-attr">serviceAccount:</span> <span class="hljs-string">csi-cinder-node-sa</span>      <span class="hljs-attr">hostNetwork:</span> <span class="hljs-literal">true</span>      <span class="hljs-attr">containers:</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">node-driver-registrar</span>          <span class="hljs-attr">image:</span> <span class="hljs-string">quay.io/k8scsi/csi-node-driver-registrar:v1.1.0</span>          <span class="hljs-attr">args:</span>            <span class="hljs-bullet">-</span> <span class="hljs-string">"--v=5"</span>            <span class="hljs-bullet">-</span> <span class="hljs-string">"--csi-address=$(ADDRESS)"</span>            <span class="hljs-bullet">-</span> <span class="hljs-string">"--kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)"</span>          <span class="hljs-attr">lifecycle:</span>            <span class="hljs-attr">preStop:</span>              <span class="hljs-attr">exec:</span>                <span class="hljs-attr">command:</span> <span class="hljs-string">["/bin/sh",</span> <span class="hljs-string">"-c"</span><span class="hljs-string">,</span> <span class="hljs-string">"rm -rf /registration/cinder.csi.openstack.org /registration/cinder.csi.openstack.org-reg.sock"</span><span class="hljs-string">]</span>          <span class="hljs-attr">env:</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">ADDRESS</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">/csi/csi.sock</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">DRIVER_REG_SOCK_PATH</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">/var/lib/kubelet/plugins/cinder.csi.openstack.org/csi.sock</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">KUBE_NODE_NAME</span>              <span class="hljs-attr">valueFrom:</span>                <span class="hljs-attr">fieldRef:</span>                  <span class="hljs-attr">fieldPath:</span> <span class="hljs-string">spec.nodeName</span>          <span class="hljs-attr">imagePullPolicy:</span> <span class="hljs-string">"IfNotPresent"</span>          <span class="hljs-attr">volumeMounts:</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">socket-dir</span>              <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/csi</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">registration-dir</span>              <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/registration</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">cinder-csi-plugin</span>          <span class="hljs-attr">securityContext:</span>            <span class="hljs-attr">privileged:</span> <span class="hljs-literal">true</span>            <span class="hljs-attr">capabilities:</span>              <span class="hljs-attr">add:</span> <span class="hljs-string">["SYS_ADMIN"]</span>            <span class="hljs-attr">allowPrivilegeEscalation:</span> <span class="hljs-literal">true</span>          <span class="hljs-attr">image:</span> <span class="hljs-string">docker.io/k8scloudprovider/cinder-csi-plugin:v1.15.0</span>          <span class="hljs-attr">args :</span>            <span class="hljs-bullet">-</span> <span class="hljs-string">/bin/cinder-csi-plugin</span>            <span class="hljs-bullet">-</span> <span class="hljs-string">"--nodeid=$(NODE_ID)"</span>            <span class="hljs-bullet">-</span> <span class="hljs-string">"--endpoint=$(CSI_ENDPOINT)"</span>            <span class="hljs-bullet">-</span> <span class="hljs-string">"--cloud-config=$(CLOUD_CONFIG)"</span>          <span class="hljs-attr">env:</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">NODE_ID</span>              <span class="hljs-attr">valueFrom:</span>                <span class="hljs-attr">fieldRef:</span>                  <span class="hljs-attr">fieldPath:</span> <span class="hljs-string">spec.nodeName</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">CSI_ENDPOINT</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">unix://csi/csi.sock</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">CLOUD_CONFIG</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">/etc/config/cloud.conf</span>          <span class="hljs-attr">imagePullPolicy:</span> <span class="hljs-string">"IfNotPresent"</span>          <span class="hljs-attr">volumeMounts:</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">socket-dir</span>              <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/csi</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">pods-mount-dir</span>              <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/var/lib/kubelet/pods</span>              <span class="hljs-attr">mountPropagation:</span> <span class="hljs-string">"Bidirectional"</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">kubelet-dir</span>              <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/var/lib/kubelet</span>              <span class="hljs-attr">mountPropagation:</span> <span class="hljs-string">"Bidirectional"</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">pods-cloud-data</span>              <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/var/lib/cloud/data</span>              <span class="hljs-attr">readOnly:</span> <span class="hljs-literal">true</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">pods-probe-dir</span>              <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/dev</span>              <span class="hljs-attr">mountPropagation:</span> <span class="hljs-string">"HostToContainer"</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">secret-cinderplugin</span>              <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/etc/config</span>              <span class="hljs-attr">readOnly:</span> <span class="hljs-literal">true</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/etc/kubernetes</span>              <span class="hljs-attr">name:</span> <span class="hljs-string">ca-cert</span>              <span class="hljs-attr">readOnly:</span> <span class="hljs-literal">true</span>      <span class="hljs-attr">volumes:</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">socket-dir</span>          <span class="hljs-attr">hostPath:</span>            <span class="hljs-attr">path:</span> <span class="hljs-string">/var/lib/kubelet/plugins/cinder.csi.openstack.org</span>            <span class="hljs-attr">type:</span> <span class="hljs-string">DirectoryOrCreate</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">registration-dir</span>          <span class="hljs-attr">hostPath:</span>            <span class="hljs-attr">path:</span> <span class="hljs-string">/var/lib/kubelet/plugins_registry/</span>            <span class="hljs-attr">type:</span> <span class="hljs-string">Directory</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">kubelet-dir</span>          <span class="hljs-attr">hostPath:</span>            <span class="hljs-attr">path:</span> <span class="hljs-string">/var/lib/kubelet</span>            <span class="hljs-attr">type:</span> <span class="hljs-string">Directory</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">pods-mount-dir</span>          <span class="hljs-attr">hostPath:</span>            <span class="hljs-attr">path:</span> <span class="hljs-string">/var/lib/kubelet/pods</span>            <span class="hljs-attr">type:</span> <span class="hljs-string">Directory</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">pods-cloud-data</span>          <span class="hljs-attr">hostPath:</span>            <span class="hljs-attr">path:</span> <span class="hljs-string">/var/lib/cloud/data</span>            <span class="hljs-attr">type:</span> <span class="hljs-string">Directory</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">pods-probe-dir</span>          <span class="hljs-attr">hostPath:</span>            <span class="hljs-attr">path:</span> <span class="hljs-string">/dev</span>            <span class="hljs-attr">type:</span> <span class="hljs-string">Directory</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">secret-cinderplugin</span>          <span class="hljs-attr">secret:</span>            <span class="hljs-attr">secretName:</span> <span class="hljs-string">cloud-config</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">ca-cert</span>          <span class="hljs-attr">secret:</span>            <span class="hljs-attr">secretName:</span> <span class="hljs-string">openstack-ca-cert</span></code></pre><p>当它们都在运行时，为Cinder创建一个 storage class 。</p><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">storage.k8s.io/v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">StorageClass</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">csi-sc-cinderplugin</span><span class="hljs-attr">provisioner:</span> <span class="hljs-string">csi-cinderplugin</span></code></pre><p>然后，我们可以使用 storage class 创建PVC。</p><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">PersistentVolumeClaim</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">myvol</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">accessModes:</span>  <span class="hljs-bullet">-</span> <span class="hljs-string">ReadWriteOnce</span>  <span class="hljs-attr">resources:</span>    <span class="hljs-string">requests:y</span>      <span class="hljs-attr">storage:</span> <span class="hljs-string">1Gi</span>  <span class="hljs-attr">storageClassName:</span> <span class="hljs-string">csi-sc-cinderplugin</span></code></pre><p>创建PVC时，将相应地创建一个Cinder卷。</p><pre><code class="hljs bash"><span class="hljs-comment"># kubectl get pvc</span>NAME    STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS          AGEmyvol   Bound    pvc-14b8bc68-6c4c-4dc6-ad79-4cb29a81faad   1Gi        RWO            csi-sc-cinderplugin   3s</code></pre><p>在OpenStack中，存储卷名称将与Kubernetes持久卷生成的名称匹配。在此示例中为：pvc-14b8bc68-6c4c-4dc6-ad79-4cb29a81faad</p><p>现在，我们可以使用PVC创建容器。</p><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">web</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">containers:</span>    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">web</span>      <span class="hljs-attr">image:</span> <span class="hljs-string">nginx</span>      <span class="hljs-attr">ports:</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">web</span>          <span class="hljs-attr">containerPort:</span> <span class="hljs-number">80</span>          <span class="hljs-attr">hostPort:</span> <span class="hljs-number">8081</span>          <span class="hljs-attr">protocol:</span> <span class="hljs-string">TCP</span>      <span class="hljs-attr">volumeMounts:</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">"/usr/share/nginx/html"</span>          <span class="hljs-attr">name:</span> <span class="hljs-string">mypd</span>  <span class="hljs-attr">volumes:</span>    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">mypd</span>      <span class="hljs-attr">persistentVolumeClaim:</span>        <span class="hljs-attr">claimName:</span> <span class="hljs-string">myvol</span></code></pre><p>当pod运行时，该存储卷将绑定到pod上。如果回到OpenStack，我们可以看到Cinder存储卷已安装到运行Pod的工作节点上。</p><pre><code class="hljs bash"><span class="hljs-comment"># openstack volume show 6b5f3296-b0eb-40cd-bd4f-2067a0d6287f</span>+--------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| Field                          | Value                                                                                                                                                                                                                                                                                                                          |+--------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| attachments                    | [&#123;u<span class="hljs-string">'server_id'</span>: u<span class="hljs-string">'1c5e1439-edfa-40ed-91fe-2a0e12bc7eb4'</span>, u<span class="hljs-string">'attachment_id'</span>: u<span class="hljs-string">'11a15b30-5c24-41d4-86d9-d92823983a32'</span>, u<span class="hljs-string">'attached_at'</span>: u<span class="hljs-string">'2019-07-24T05:02:34.000000'</span>, u<span class="hljs-string">'host_name'</span>: u<span class="hljs-string">'compute-6'</span>, u<span class="hljs-string">'volume_id'</span>: u<span class="hljs-string">'6b5f3296-b0eb-40cd-bd4f-2067a0d6287f'</span>, u<span class="hljs-string">'device'</span>: u<span class="hljs-string">'/dev/vdb'</span>, u<span class="hljs-string">'id'</span>: u<span class="hljs-string">'6b5f3296-b0eb-40cd-bd4f-2067a0d6287f'</span>&#125;] || availability_zone              | nova                                                                                                                                                                                                                                                                                                                           || bootable                       | <span class="hljs-literal">false</span>                                                                                                                                                                                                                                                                                                                          || consistencygroup_id            | None                                                                                                                                                                                                                                                                                                                           || created_at                     | 2019-07-24T05:02:18.000000                                                                                                                                                                                                                                                                                                     || description                    | Created by OpenStack Cinder CSI driver                                                                                                                                                                                                                                                                                         || encrypted                      | False                                                                                                                                                                                                                                                                                                                          || id                             | 6b5f3296-b0eb-40cd-bd4f-2067a0d6287f                                                                                                                                                                                                                                                                                           || migration_status               | None                                                                                                                                                                                                                                                                                                                           || multiattach                    | False                                                                                                                                                                                                                                                                                                                          || name                           | pvc-14b8bc68-6c4c-4dc6-ad79-4cb29a81faad                                                                                                                                                                                                                                                                                       || os-vol-host-attr:host          | rbd:volumes@rbd<span class="hljs-comment">#rbd                                                                                                                                                                                                                                                                                                            |</span>| os-vol-mig-status-attr:migstat | None                                                                                                                                                                                                                                                                                                                           || os-vol-mig-status-attr:name_id | None                                                                                                                                                                                                                                                                                                                           || os-vol-tenant-attr:tenant_id   | 14ba698c0aec4fd6b7dc8c310f664009                                                                                                                                                                                                                                                                                               || properties                     | attached_mode=<span class="hljs-string">'rw'</span>, cinder.csi.openstack.org/cluster=<span class="hljs-string">'kubernetes'</span>                                                                                                                                                                                                                                                              || replication_status             | None                                                                                                                                                                                                                                                                                                                           || size                           | 1                                                                                                                                                                                                                                                                                                                              || snapshot_id                    | None                                                                                                                                                                                                                                                                                                                           || source_volid                   | None                                                                                                                                                                                                                                                                                                                           || status                         | <span class="hljs-keyword">in</span>-use                                                                                                                                                                                                                                                                                                                         || <span class="hljs-built_in">type</span>                           | rbd                                                                                                                                                                                                                                                                                                                            || updated_at                     | 2019-07-24T05:02:35.000000                                                                                                                                                                                                                                                                                                     || user_id                        | 5f6a7a06f4e3456c890130d56babf591                                                                                                                                                                                                                                                                                               |+--------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</code></pre><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>在本演练中，我们在OpenStack VM上部署了一个Kubernetes集群，并与外部OpenStack云厂商集成。然后，在此Kubernetes集群上，我们部署了Cinder CSI插件，该插件可以创建Cinder存储卷，并将它们作为持久卷在Kubernetes中使用。</p><p>作者：王延飞</p>]]></content>
    
    
    
    <tags>
      
      <tag>openstack, K8s, kubeadm, CentOS</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>你的Kubernetes集群健康吗?这里有5种检验方法</title>
    <link href="/2020/05/29/%E4%BD%A0%E7%9A%84Kubernetes%E9%9B%86%E7%BE%A4%E5%81%A5%E5%BA%B7%E5%90%97-%E8%BF%99%E9%87%8C%E6%9C%895%E7%A7%8D%E6%A3%80%E9%AA%8C%E6%96%B9%E6%B3%95/"/>
    <url>/2020/05/29/%E4%BD%A0%E7%9A%84Kubernetes%E9%9B%86%E7%BE%A4%E5%81%A5%E5%BA%B7%E5%90%97-%E8%BF%99%E9%87%8C%E6%9C%895%E7%A7%8D%E6%A3%80%E9%AA%8C%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<p>Kubernetes 是一门极其智能化的科学技术，但是如果搞错了方向，它就会以一种非预期的方式响应. 对于大部分 “聪明的” 科技来说， 其智能化程度取决于操作者。 为了通过使用Kubernetes组建团队来达到成功巅峰，其中最关键的是每一位组员能清晰地掌握Kubernetes 集群。这里有5种方法可以使工程师在建立Kubernetes集群时能最好的鉴定出任何未结的任务，并且尽可能确保负载在最健康的水平（想要更深入的了解 Kubernetes 的可观察性，请查阅电子书 <a href="https://www.sumologic.com/brief/kubernetes-observability/" target="_blank" rel="noopener">Kubernetes Observability</a>）。</p><p>  幸运的是， 在 Kubernetes 环境中有技术可以收集日志、性能指标、事件和安全警告，从而帮助我们模拟各种集群的健康水平。这些收集器从Kubernetes 集群的所有组件中收集数据，它们汇集起来可以得到一个 集群健康状况的 高层级的视图， 并且可以及时深入的了解诸如 资源（resource）， 使用性能(utilization)， 配置(configuration)错误和其他问题。</p><p> <strong>1. 在所有的pod</strong> <strong>库中设置 CPU requests （请求）和 limits （限额）</strong></p><p> requests（请求）和limits（限额）是k8s的一种机制，该机制用来给pods 自动分配 像CPU和内存使用量这样的可用资源 。CPU 是用毫核来定义的， 所以1000毫核等于1核。requests（请求）是预期给定容器所需要的资源使用量，limits（限额）， 换句话说，就是一个容器被允许使用的资源量的实际上限。 </p><p><img src="https://cdn.thenewstack.io/media/2020/01/d26b69ba-sumo1.png" srcset="/img/loading.gif" alt="img"></p><p>确保所有的pods 都指定requests（请求）。一个最佳的实践是给这些pods 指定1核或者更少，如果需要更多算力的话，就添加额外的副本。还有很重要的需要注意的一点， 如果你配置的太高， 比如2000毫核（2核），但是只有1核可用，那么这个pod将永远不会被调度。 在第5步中，我会告诉你如何去重复确认哪些未被调度的pods。</p><p> 确保所有的pods 都有CPU limits（限额）。 如上面提到的，limits（限额）是上限， 所以Kubernetes不允许pod 使用超出limits（限额）所定义的CPU 数量。即便如此，CPU 有点宽容，因为它被认为是一种可压缩资源。如果pod 达到CPU limits（限额）， 它将不会被终止， 而是被限制。你的CPU将被限制，所以你可能会遇到性能问题。</p><p> <strong>2.</strong> <strong>在所有的</strong> <strong>pods</strong> <strong>上设置内存</strong> <strong>requests</strong> <strong>（请求）和</strong> <strong>limits</strong> <strong>（限额）</strong> </p><p><strong><img src="https://cdn.thenewstack.io/media/2020/01/704d2d55-sumo2.png" srcset="/img/loading.gif" alt="img"></strong></p><p> 确保为所有pod设置了内存requests（请求）：内存requests（请求）是指你认为pod将消耗多少数据。与CPU一样，如果内存requests（请求）大于最大节点，Kubernetes也不会调度pod。</p><p>  确保为所有pod设置了内存limits（限额）：内存limits（限额）是允许pod使用多少内存的硬上限。与CPU不同，内存是不可压缩的，不能被限制。如果一个容器超过了它的内存limits（限额），那么它将被终止。</p><p><strong>3.</strong> <strong>审核资源配额</strong></p><p>  Kubernetes的最佳健康状况的另一检查项是— 是否不足或过度配置资源。如果可用CPU和内存过剩，那么消耗不足,很可能有浪费。另一方面，如果接近100%的利用率，则在需要扩展或有意外负载时可能会遇到问题。</p><p>  检查剩余的pod容量。一个有用的Kubernete衡量标准是“kube_node_status_allocatable”，这是Kubernetes在给定平均pod资源利用率的情况下，对一个节点将容纳多少个pod的估计。我们可以把剩余的pod容量加起来，粗略估计一下在不遇到问题的情况下可以扩展多少。<img src="https://cdn.thenewstack.io/media/2020/01/7a0161ca-screen-shot-2020-01-08-at-2.27.59-pm.png" srcset="/img/loading.gif" alt="img"> </p><p>检查总的CPU使用率百分比、请求的CPU百分比与CPU limits（限额）百分比：总的CPU使用率表示现在使用了多少，requests（请求）表示预计可能需要多少，而limits（限额）是我们设置的硬性上限。</p><p>  在下面的例子中，我们只使用了可用算力的2.5%，供应远超实际使用量，而且还可能会减少。相比之下，我们的CPUrequests（请求）是46%，因此我们原以为我们可能需要用到的比实际使用的要多得多。（出现这种情况）要么是预估错误，要么就是有计划以外的突发需求。</p><p>  最后，我们的CPU Burstable告诉我们所有的CPU limits（限额）总和。由于其低于requests（请求）的CPU资源量，我们可能要回去检查我们的限制设置。要么是我们对每一个pod都没有设置limits（限额），要么我们的limits（限额）配置错误。<img src="https://cdn.thenewstack.io/media/2020/01/76a8f135-screen-shot-2020-01-08-at-2.27.19-pm.png" srcset="/img/loading.gif" alt="img"></p><p>  检查总内存使用量百分比、内存requests（请求）百分比与内存limits（限额）百分比。就和检查CPU一样，我们可以检查内存是否配置过大。仅仅3.8%的利用率告诉我们，我们的确实供应过剩了，但我们可以舒适地长期扩展。 </p><p><img src="https://cdn.thenewstack.io/media/2020/01/f25aa68f-screen-shot-2020-01-08-at-2.27.31-pm.png" srcset="/img/loading.gif" alt="img"></p><p><strong>4.</strong> <strong>检查节点间的</strong> <strong>Pod</strong> <strong>分布</strong></p><p>  当我们研究pod是如何在集群中的可用节点上分配时，我们想要一个大致均等的分配。如果某些节点完全超载或欠载，这可能意味着一个值得调查的更大问题出现了。</p><p>  需要检查的一些可能导致分配不均的因素包括：</p><p>  节点亲和力（node affinity）。 亲和力（Affinity）是一个pod设置项，使它们偏好具有某些属性的节点。例如，某些pod可能需要在带有GPU或SSD的计算机上运行，或者某些pod可能需要具有特定安全隔离或特定策略的节点。重复检查关联设置可以帮助缩小 导致不均匀分配的原因的范围，并降低严峻问题发生的可能性。</p><p>  污点（taints）和容忍(tolerations)。污点与亲和力相反。这些是节点的设置项，给节点设置污点，从而使pod 难以调度到这些节点。如果要为特定pod保留节点，或者为了确保该节点上的pod对可用资源具有完全访问权限，则可以使用此选项。</p><p><img src="https://cdn.thenewstack.io/media/2020/01/aacfa6df-sumo7.png" srcset="/img/loading.gif" alt="img">   </p><p>limits（限额）和requests（请求）：回顾limits（限额）和requests（请求）设置项。这常常是问题的起因，以至于它值得在本文的三个部分中提及。如果调度器没有关于pods需要什么的正确信息，那么它的调度工作将会做的很差。</p><p>  <strong>5.</strong> <strong>检查</strong> <strong>pod</strong> <strong>是否处于不良状态</strong></p><p>在Kubernetes环境中，当前的状态每时每刻都在变化，因此过度关注每一个终止的pod会慢慢侵蚀你的时间和理智。然而，为了确保它与您基于当前集群中的事件来作出的预期相符。以下所列项值得关注。 <img src="https://cdn.thenewstack.io/media/2020/01/d710f4dc-screen-shot-2020-01-08-at-2.27.45-pm.png" srcset="/img/loading.gif" alt="img"></p><p> <strong>*Nodes not ready</strong>：节点可能由于多种原因而进入此状态，但通常是因为内存或磁盘空间不足。</p><p> <strong>*Unscheduled pods</strong>：pod通常会由于调度器无法满足CPU或内存requests（请求） 而以未被调度的状态终止。重复检查你的集群是否有足够的podsrequests（请求）的可用资源。</p><p>  <strong>*Pods that failed to create</strong> :Pods在创建时失败，通常是因为镜像问题，比如启动脚本依赖项缺失。这种情况，建议回到原点。</p><p>  <strong>*Container restarts</strong>：只有一部分容器重启是不用担心的，但当你看到大量的（容器重启）则可能意味着pod处于OOMKill（内存不足而被杀死）状态。内存不足是Kubernetes中最常见的错误（error）之一，可能是由于镜像问题、及其引发的依赖问题或意外、limits（限额）和requests（请求）问题引起的。</p><p>  这些集群健康的最佳实践可以限制Kubernetes环境中的意外行为，并确保您不会在将来遇到可伸缩性问题。这些也是帮你回答那些无定形问题的一个起点，比如，“我的Kubernetes集群健康吗？”，如果所有这些项都是绿色的，那么您的集群可能非常健康，那么您可以轻松休息了。</p><p>  想要更深入的了解 Kubernetes 的可观察性，请查阅电子书 <a href="https://www.sumologic.com/brief/kubernetes-observability/" target="_blank" rel="noopener">Kubernetes Observability</a></p><p>作者：wangjing</p>]]></content>
    
    
    
    <tags>
      
      <tag>cloudnative, K8s, test, Pod</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>使用Spring Boot Operator部署SpringBoot到K8S</title>
    <link href="/2020/05/28/%E4%BD%BF%E7%94%A8Spring-Boot-Operator%E9%83%A8%E7%BD%B2SpringBoot%E5%88%B0K8S/"/>
    <url>/2020/05/28/%E4%BD%BF%E7%94%A8Spring-Boot-Operator%E9%83%A8%E7%BD%B2SpringBoot%E5%88%B0K8S/</url>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>在Kubernetes中部署spring boot应用整体上来说是一件比较繁琐的事情，而<a href="https://github.com/goudai/spring-boot-operator" target="_blank" rel="noopener">Spring Boot Operator</a>则能带给你更清爽简单的体验。</p><p><a href="https://github.com/goudai/spring-boot-operator" target="_blank" rel="noopener">Spring Boot Operator</a>基于Kubernetes的<a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/extend-api-custom-resource-definitions" target="_blank" rel="noopener">custom resource definitions (CRDs)</a>扩展API进行的开发。</p><h2 id="打包Docker镜像"><a href="#打包Docker镜像" class="headerlink" title="打包Docker镜像"></a>打包Docker镜像</h2><p>在讲部署之前我们需要先将我们的SpringBoot应用打包成标准的DockerImage。</p><p>java项目打包镜像用maven/gradle插件比较多，我的另一篇文章<a href="https://qingmu.io/2018/08/07/How-to-run-springcloud-in-docker/" target="_blank" rel="noopener">构建SpringBoot的Docker镜像</a>，这里在介绍一个新的google开源的插件<a href="https://github.com/GoogleContainerTools/jib" target="_blank" rel="noopener">Jib</a>，该插件使用起来比较方便。</p><p>注意：jib打包的镜像会导致java应用的pid=1，在使用SpringBootOperator进行发布时候，Operator会设置kubernetes的<a href="https://kubernetes.io/docs/tasks/configure-pod-container/share-process-namespace/" target="_blank" rel="noopener">ShareProcessNamespace</a>参数为true（v1.10+版本都可使用）来解决该问题。</p><p>下面就来演示一下我们通过<a href="https://start.spring.io/" target="_blank" rel="noopener">https://start.spring.io</a>生成一个标准的SpringBoot项目<a href="https://github.com/goudai/operator-demo" target="_blank" rel="noopener">operator-demo</a>,然后使用jib插件进行镜像打包</p><pre><code class="hljs shell">mvn com.google.cloud.tools:jib-maven-plugin:build \-Djib.to.auth.username=$&#123;&#123; secrets.MY_USERNAME &#125;&#125; \-Djib.to.auth.password=$&#123;&#123; secrets.MY_PASSWORD &#125;&#125; \-Djib.container.jvmFlags=--add-opens,java.base/sun.nio.ch=ALL-UNNAMED \-Djib.from.image=freemanliu/oprenjre:11.0.5 \-Dimage=registry.cn-shanghai.aliyuncs.com/qingmuio/operator-demo/operator-demo:v1.0.0</code></pre><p>执行上面的命令之后我们将得到一个标准的docker镜像，该镜像会被推送到远程仓库。</p><h2 id="Operator快速体验"><a href="#Operator快速体验" class="headerlink" title="Operator快速体验"></a>Operator快速体验</h2><p>完成了镜像的构建之后,我们紧接着来安装我们的Operator到kubernetes集群，当然了首先你需要一套集群，可以参考我之前一篇文章<a href="https://qingmu.io/2019/05/17/Deploy-a-highly-available-cluster-with-kubeadm/" target="_blank" rel="noopener">部署高可用kubernetes</a>，虽然版本比较老,但是新版本其实也差不多的一个思路。</p><h3 id="快速安装"><a href="#快速安装" class="headerlink" title="快速安装"></a>快速安装</h3><p>此处快速安装只是为了快速体验demo</p><pre><code class="hljs sh">kubectl apply -f https://raw.githubusercontent.com/goudai/spring-boot-operator/master/manifests/deployment.yaml</code></pre><p>apply成功之后控制台输出</p><pre><code class="hljs shell">namespace/spring-boot-operator-system createdcustomresourcedefinition.apiextensions.k8s.io/springbootapplications.springboot.qingmu.io createdrole.rbac.authorization.k8s.io/spring-boot-operator-leader-election-role createdclusterrole.rbac.authorization.k8s.io/spring-boot-operator-manager-role createdclusterrole.rbac.authorization.k8s.io/spring-boot-operator-proxy-role createdclusterrole.rbac.authorization.k8s.io/spring-boot-operator-metrics-reader createdrolebinding.rbac.authorization.k8s.io/spring-boot-operator-leader-election-rolebinding createdclusterrolebinding.rbac.authorization.k8s.io/spring-boot-operator-manager-rolebinding createdclusterrolebinding.rbac.authorization.k8s.io/spring-boot-operator-proxy-rolebinding createdservice/spring-boot-operator-controller-manager-metrics-service createddeployment.apps/spring-boot-operator-controller-manager created</code></pre><p>稍等片刻查看是否已经安装成功</p><pre><code class="hljs shell">kubectl  get po -n spring-boot-operator-system</code></pre><p>成功如下输出</p><pre><code class="hljs shell">NAME                                                       READY   STATUS    RESTARTS   AGEspring-boot-operator-controller-manager-7f498596bb-wcwtn   2/2     Running   0          2m15s</code></pre><h4 id="部署OperatorDemo应用"><a href="#部署OperatorDemo应用" class="headerlink" title="部署OperatorDemo应用"></a>部署OperatorDemo应用</h4><p>完成了Operator的部署之后，我们来部署我们第一个应用，这里我们就发布上面我们编写的springboot应用opreator-demo。<br>首先我们需要先编写一个Spring Boot Application 的CRD部署yaml，如下</p><pre><code class="hljs yaml"><span class="hljs-comment"># Demo.yaml</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">springboot.qingmu.io/v1alpha1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">SpringBootApplication</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">operator-demo</span> <span class="hljs-attr">spec:</span>  <span class="hljs-attr">springBoot:</span>    <span class="hljs-attr">version:</span> <span class="hljs-string">v1.0.0</span><span class="hljs-comment">#    image: registry.cn-shanghai.aliyuncs.com/qingmuio/operator-demo/operator-demo:v1.0.0</span></code></pre><p>细心的同学可能发现了，为啥连<code>Image</code>都没有？这怎么发布，就name，version，就能完成发布？是的没错！就能完成发布，后面我讲详细讲到他是如何完成的。<br>接着我们apply一下</p><pre><code class="hljs shell">kubectl apply -f Demo.yaml</code></pre><p>看到console输出</p><pre><code class="hljs shell">springbootapplication.springboot.qingmu.io/operator-demo created</code></pre><h4 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h4><p>表示创建成功了，接着我们来看下我们部署的第一个应用，这里我们直接用上面的yaml中的name过滤即可。<br>查看pod</p><pre><code class="hljs shell">~# kubectl  get po | grep operator-demooperator-demo-7574f4789c-mg58m             1/1     Running   0          76soperator-demo-7574f4789c-ssr8v             1/1     Running   0          76soperator-demo-7574f4789c-sznww             1/1     Running   0          76s</code></pre><p>查看下我们的pid不等于1的设置是否生效,根据下面的结果可以看到通过设置<a href="https://kubernetes.io/docs/tasks/configure-pod-container/share-process-namespace/" target="_blank" rel="noopener">ShareProcessNamespace</a>该参数我们可以在Kubernetes层面来解决这个pid=1的问题。</p><pre><code class="hljs shell">kubectl exec -it operator-demo-7574f4789c-mg58m bashbash-5.0# ps -efUID        PID  PPID  C STIME TTY          TIME CMDroot         1     0  0 02:06 ?        00:00:00 /pauseroot         6     0 26 02:06 ?        00:00:09 java --add-opens java.base/sun.nio.ch=ALL-UNNAMED -cp /app/resources:/app/classes:/app/libs/* io.qingmu.operator.operatordemo.Oper...root        38     0  0 02:07 pts/0    00:00:00 bashroot        44    38  0 02:07 pts/0    00:00:00 ps -ef</code></pre><p>查看svc</p><pre><code class="hljs shell">~# kubectl  get svc | grep operator-demooperator-demo             ClusterIP   10.101.128.6     &lt;none&gt;        8080/TCP            2m52s</code></pre><p>我们来访问一下试试。</p><pre><code class="hljs shell">root@server1:~# curl -i http://10.101.128.6:8080HTTP/1.1 200 Content-Type: text/plain;charset=UTF-8Content-Length: 9Date: Wed, 08 Apr 2020 08:45:46 GMThello !!!</code></pre><p>我们来试着缩减他的副本数到1个<br>编辑我们的Demo.yaml，加入一个新的属性<code>replicas</code></p><pre><code class="hljs yaml"><span class="hljs-comment"># Demo.yaml</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">springboot.qingmu.io/v1alpha1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">SpringBootApplication</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">operator-demo</span> <span class="hljs-attr">spec:</span>  <span class="hljs-attr">springBoot:</span>    <span class="hljs-attr">version:</span> <span class="hljs-string">v1.0.0</span>    <span class="hljs-attr">replicas:</span> <span class="hljs-number">1</span></code></pre><p>应用一下</p><pre><code class="hljs shell">root@server1:~# kubectl apply -f Demo.yaml springbootapplication.springboot.qingmu.io/operator-demo configured</code></pre><p>再次查看pod，你会发现我们的pod已经缩放为一个副本了</p><pre><code class="hljs shell">~# kubectl  get po | grep operator-demooperator-demo-7574f4789c-sznww             1/1     Running   0          8m29s</code></pre><h4 id="清理operator-demo"><a href="#清理operator-demo" class="headerlink" title="清理operator-demo"></a>清理operator-demo</h4><p>要删除该pod 我们只需要执行delete即可</p><pre><code class="hljs shell">~# kubectl delete -f Demo.yaml springbootapplication.springboot.qingmu.io "operator-demo" deleted</code></pre><p>再次查看pod，已经没了</p><pre><code class="hljs shell">kubectl  get po | grep operator-demo</code></pre><h2 id="部署自己的应用"><a href="#部署自己的应用" class="headerlink" title="部署自己的应用"></a>部署自己的应用</h2><p>部署自己私有仓库的应用需要需要先创建secret(如果已经创建跳过即可)<br>创建docker-registry的secret</p><pre><code class="hljs shell">kubectl create  \secret docker-registry aliyun-registry-secret \--docker-server=registry-vpc.cn-hangzhou.aliyuncs.com \--docker-username=*** \--docker-password=*** \--docker-email=***</code></pre><p>自己应用的crd Yaml</p><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">springboot.qingmu.io/v1alpha1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">SpringBootApplication</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">你的应用的名称</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">springBoot:</span>    <span class="hljs-attr">version:</span> <span class="hljs-string">v1.0.0</span>    <span class="hljs-attr">replicas:</span> <span class="hljs-number">1</span>     <span class="hljs-attr">image:</span> <span class="hljs-string">你的image地址</span>    <span class="hljs-attr">imagePullSecrets:</span>       <span class="hljs-bullet">-</span> <span class="hljs-string">上面创建的secret</span></code></pre><h1 id="一个完整的Spring-Boot-Application-Yaml"><a href="#一个完整的Spring-Boot-Application-Yaml" class="headerlink" title="一个完整的Spring Boot Application Yaml"></a>一个完整的Spring Boot Application Yaml</h1><p>下面是一个完整的yaml属性结构，大部分属性我们都可以用默认配置的即可。<br>不设置属性，默认使用Operator中设置的通用值详见后面的自定义安装Operator。</p><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">springboot.qingmu.io/v1alpha1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">SpringBootApplication</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">operator-demo</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">default</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">springBoot:</span>    <span class="hljs-comment"># image 可以不设置，如果不设置默认使用 IMAGE_REPOSITORY+/+mate.name+:+spec.springBoot.version</span>    <span class="hljs-comment"># registry.cn-shanghai.aliyuncs.com/qingmuio + / + operator-demo + : + v1.0.0</span>    <span class="hljs-attr">image:</span> <span class="hljs-string">registry.cn-shanghai.aliyuncs.com/qingmuio/operator-demo:v1.0.0</span>    <span class="hljs-attr">clusterIp:</span> <span class="hljs-string">""</span>     <span class="hljs-attr">version:</span> <span class="hljs-string">v1.0.0</span>     <span class="hljs-attr">replicas:</span> <span class="hljs-number">1</span>     <span class="hljs-attr">resource:</span>      <span class="hljs-attr">cpu:</span>        <span class="hljs-attr">request:</span> <span class="hljs-string">50m</span>        <span class="hljs-attr">limit:</span> <span class="hljs-string">""</span>       <span class="hljs-attr">memory:</span>        <span class="hljs-attr">request:</span> <span class="hljs-string">1Gi</span>        <span class="hljs-attr">limit:</span> <span class="hljs-string">1Gi</span>     <span class="hljs-attr">path:</span>      <span class="hljs-attr">liveness:</span> <span class="hljs-string">/actuator/health</span>       <span class="hljs-attr">readiness:</span> <span class="hljs-string">/actuator/health</span>       <span class="hljs-attr">hostLog:</span> <span class="hljs-string">/var/applog</span>       <span class="hljs-attr">shutdown:</span> <span class="hljs-string">/spring/shutdown</span>     <span class="hljs-attr">imagePullSecrets:</span>       <span class="hljs-bullet">-</span> <span class="hljs-string">aliyun-docker-registry-secret</span>    <span class="hljs-attr">env:</span>       <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">EUREKA_SERVERS</span>        <span class="hljs-attr">value:</span> <span class="hljs-string">http://eureka1:8761/eureka/,http://eureka2:8761/eureka/,http://eureka3:8761/eureka/</span>    <span class="hljs-attr">nodeAffinity:</span>       <span class="hljs-attr">key:</span> <span class="hljs-string">"failure-domain.beta.kubernetes.io/zone"</span>      <span class="hljs-attr">operator:</span> <span class="hljs-string">"In"</span>      <span class="hljs-attr">values:</span>        <span class="hljs-bullet">-</span> <span class="hljs-string">"cn-i"</span>        <span class="hljs-bullet">-</span> <span class="hljs-string">"cn-h"</span>        <span class="hljs-bullet">-</span> <span class="hljs-string">"cn-g"</span></code></pre><h4 id="优雅停机的路径"><a href="#优雅停机的路径" class="headerlink" title="优雅停机的路径"></a>优雅停机的路径</h4><p>由于优雅停机默认是关闭的并且并不支持Get请求所以我们需要开启和搭个桥<br>首先在<code>application.yml</code>中启用</p><pre><code class="hljs yaml"><span class="hljs-attr">management:</span>  <span class="hljs-attr">endpoints:</span>    <span class="hljs-attr">web:</span>      <span class="hljs-attr">exposure:</span>        <span class="hljs-attr">include:</span> <span class="hljs-string">"*"</span>  <span class="hljs-attr">endpoint:</span>    <span class="hljs-attr">shutdown:</span>      <span class="hljs-attr">enabled:</span> <span class="hljs-literal">true</span></code></pre><p>然后桥接一个Get方法</p><pre><code class="hljs java"><span class="hljs-meta">@RestController</span><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ShutdownController</span> </span>&#123;    <span class="hljs-meta">@Autowired</span>    <span class="hljs-keyword">private</span> ShutdownEndpoint shutdownEndpoint;    <span class="hljs-meta">@GetMapping</span>(<span class="hljs-string">"/spring/shutdown"</span>)    <span class="hljs-function"><span class="hljs-keyword">public</span> Map&lt;String, String&gt; <span class="hljs-title">shutdown</span><span class="hljs-params">(HttpServletRequest request)</span> </span>&#123;        <span class="hljs-keyword">return</span> shutdownEndpoint.shutdown();    &#125;&#125;</code></pre><h3 id="node亲和的使用"><a href="#node亲和的使用" class="headerlink" title="node亲和的使用"></a>node亲和的使用</h3><p>举一个列子 我们有一个springboot应用 user-service 希望他能分布到3个可用区的6个节点上:<br>首先我们把机器划分多个可用区</p><pre><code class="hljs java">cn-i区(node-i1,node-i02)cn-h区(node-g1,node-g02)cn-g区(node-h1,node-h02)</code></pre><p>现在我们有三个可以区 每个区有2台workload，一共6台。然后我们需要给这些机器分别打上label。<br>将全部的i区机器标注为cn-i</p><pre><code class="hljs java">kubectl label node node-i1 failure-domain.beta.kubernetes.io/zone=cn-ikubectl label node node-i2 failure-domain.beta.kubernetes.io/zone=cn-i</code></pre><p>同理将h区的标注为h，g区同理</p><pre><code class="hljs java">kubectl label node node-h1 failure-domain.beta.kubernetes.io/zone=cn-ikubectl label node node-ih2 failure-domain.beta.kubernetes.io/zone=cn-i</code></pre><p>现在准备工作我们就绪了，现在我们来设置让它达到我们的调度效果，像如下编写即可。</p><pre><code class="hljs java">spec:  springBoot:    nodeAffinity: #可以不设置 节点亲和 这里演示的是尽量将pod分散到 i h g 三个可用区，默认设置了pod反亲和      key: <span class="hljs-string">"failure-domain.beta.kubernetes.io/zone"</span>      operator: <span class="hljs-string">"In"</span>      values:        - <span class="hljs-string">"cn-i"</span>        - <span class="hljs-string">"cn-h"</span>        - <span class="hljs-string">"cn-g"</span></code></pre><h2 id="Operator-自定义安装"><a href="#Operator-自定义安装" class="headerlink" title="Operator 自定义安装"></a>Operator 自定义安装</h2><p>上面我们快速的安装了好了，接着我们来讲解下如何自定义安装，以及有哪些自定义的参数，可以个性化的参数我们用环境变量的方式注入。<br>下面来修改<code>Deployment</code>完成自己个性化的配置部署，从我提供的部署yaml中拉倒最后，找到name是<code>spring-boot-operator-controller-manager</code>的Deployment，我们将修改它。</p><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">apps/v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Deployment</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">control-plane:</span> <span class="hljs-string">controller-manager</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">spring-boot-operator-controller-manager</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">spring-boot-operator-system</span><span class="hljs-string">.....</span>                <span class="hljs-comment">#注意：一下配置针对通用全局的spring boot默认配置，对crd的spring boot生效，这里不配置也可以在部署的yaml中指定</span>        <span class="hljs-comment"># 私有仓库的地址，比如我的最终打包的镜像地址是 registry.cn-shanghai.aliyuncs.com/qingmuio/operator-demo/operator-demo:v1.0.0</span>        <span class="hljs-comment"># 那么配置的值是 registry.cn-shanghai.aliyuncs.com/qingmuio/operator-demo</span>        <span class="hljs-comment"># 配置这个值之后，我们我们如果在发布的yaml中不写image，那么使用的image就是 IMAGE_REPOSITORY+"/"+mate.name+spec.springBoot.version</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">IMAGE_REPOSITORY</span>          <span class="hljs-attr">value:</span> <span class="hljs-string">registry.cn-shanghai.aliyuncs.com/qingmuio</span>        <span class="hljs-comment"># 请求CPU限制</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">REQUEST_CPU</span>          <span class="hljs-attr">value:</span> <span class="hljs-string">50m</span>        <span class="hljs-comment"># 限制最大能用最大CPU java应用可以不用限制，限制不合理会导致启动异常缓慢</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">LIMIT_CPU</span>          <span class="hljs-attr">value:</span> <span class="hljs-string">""</span>        <span class="hljs-comment"># 请求内存大小</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">REQUEST_MEMORY</span>          <span class="hljs-attr">value:</span> <span class="hljs-string">500Mi</span>        <span class="hljs-comment"># 限制最大内存大小 一般和request一样大即可</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">LIMIT_MEMORY</span>          <span class="hljs-attr">value:</span> <span class="hljs-string">500Mi</span>        <span class="hljs-comment"># 就绪检查Path，spring boot actuator 默认Path</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">READINESS_PATH</span>          <span class="hljs-attr">value:</span> <span class="hljs-string">/actuator/health</span>        <span class="hljs-comment"># 就绪存活Path，spring boot actuator 默认Path</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">LIVENESS_PATH</span>          <span class="hljs-attr">value:</span> <span class="hljs-string">/actuator/health</span>        <span class="hljs-comment"># 就绪存活Path，优雅停机Path</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">SHUTDOWN_PATH</span>          <span class="hljs-attr">value:</span> <span class="hljs-string">/spring/shutdown</span>        <span class="hljs-comment"># 复制级 即副本数</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">REPLICAS</span>          <span class="hljs-attr">value:</span> <span class="hljs-string">"3"</span>        <span class="hljs-comment"># 将日志外挂到主机磁盘Path，默认两者相同</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">HOST_LOG_PATH</span>          <span class="hljs-attr">value:</span> <span class="hljs-string">/var/applog</span>        <span class="hljs-comment"># 用于pull 镜像的secrets</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">IMAGE_PULL_SECRETS</span>          <span class="hljs-attr">value:</span> <span class="hljs-string">""</span>        <span class="hljs-comment"># 用于pull 镜像的secrets</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">SPRING_BOOT_DEFAULT_PORT</span>          <span class="hljs-attr">value:</span> <span class="hljs-string">"8080"</span>        <span class="hljs-comment"># node亲和，比如我可以设置pod尽量分散在不同可用区cn-i,cn-g,cn-h区</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">NODE_AFFINITY_KEY</span>          <span class="hljs-attr">value:</span> <span class="hljs-string">""</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">NODE_AFFINITY_OPERATOR</span>          <span class="hljs-attr">value:</span> <span class="hljs-string">""</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">NODE_AFFINITY_VALUES</span>          <span class="hljs-attr">value:</span> <span class="hljs-string">""</span>        <span class="hljs-comment"># 全局的环境变量，会追加到每个spring boot的每个pod中，格式 k=v;k1=v2,</span>        <span class="hljs-comment"># 如 EUREKA_SERVERS=http://eureka1:8761/eureka/,http://eureka2:8761/eureka/,http://eureka3:8761/eureka/;k=v</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">SPRING_BOOT_ENV</span>          <span class="hljs-attr">value:</span> <span class="hljs-string">""</span>        <span class="hljs-attr">image:</span> <span class="hljs-string">registry.cn-shanghai.aliyuncs.com/qingmuio/spring-boot-operator-controller:latest</span><span class="hljs-string">.....</span></code></pre><h3 id="自定义安装之后部署"><a href="#自定义安装之后部署" class="headerlink" title="自定义安装之后部署"></a>自定义安装之后部署</h3><p>yaml可以简化为如下。</p><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">springboot.qingmu.io/v1alpha1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">SpringBootApplication</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">你的应用的名称</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">springBoot:</span>    <span class="hljs-attr">version:</span> <span class="hljs-string">v1.0.0</span></code></pre><h1 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h1><p>环境变量表格</p><table><thead><tr><th align="center">环境变量名</th><th align="center">是否可以空</th><th align="center">默认值</th><th align="center">说明</th></tr></thead><tbody><tr><td align="center">IMAGE_REPOSITORY</td><td align="center">true</td><td align="center">“”</td><td align="center">私有仓库的地址</td></tr><tr><td align="center">REQUEST_CPU</td><td align="center">true</td><td align="center">50m</td><td align="center">请求CPU限制</td></tr><tr><td align="center">LIMIT_CPU</td><td align="center">true</td><td align="center">“”</td><td align="center">限制最大能用最大CPU java应用可以不用限制，限制不合理会导致启动异常缓慢</td></tr><tr><td align="center">REQUEST_MEMORY</td><td align="center">true</td><td align="center">2Gi</td><td align="center">请求内存大小</td></tr><tr><td align="center">LIMIT_MEMORY</td><td align="center">true</td><td align="center">2Gi</td><td align="center">限制最大内存大小 一般和request一样大即可</td></tr><tr><td align="center">READINESS_PATH</td><td align="center">true</td><td align="center">/actuator/health</td><td align="center">就绪检查Path，spring boot actuator 默认Path</td></tr><tr><td align="center">LIVENESS_PATH</td><td align="center">true</td><td align="center">/actuator/health</td><td align="center">存活检查Path，spring boot actuator 默认Path</td></tr><tr><td align="center">SHUTDOWN_PATH</td><td align="center">true</td><td align="center">/spring/shutdown</td><td align="center">就绪存活Path，优雅停机Path</td></tr><tr><td align="center">REPLICAS</td><td align="center">true</td><td align="center">3</td><td align="center">副本数</td></tr><tr><td align="center">HOST_LOG_PATH</td><td align="center">true</td><td align="center">/var/applog</td><td align="center">将日志外挂到主机磁盘Path，默认两者相同</td></tr><tr><td align="center">IMAGE_PULL_SECRETS</td><td align="center">true</td><td align="center">无</td><td align="center">用于pull 镜像的secrets</td></tr><tr><td align="center">SPRING_BOOT_DEFAULT_PORT</td><td align="center">true</td><td align="center">8080</td><td align="center">用于pull 镜像的secrets</td></tr><tr><td align="center">NODE_AFFINITY_KEY</td><td align="center">true</td><td align="center">“”</td><td align="center">node亲和key，比如我可以设置pod尽量分散在不同可用区cn-i,cn-g,cn-h区</td></tr><tr><td align="center">NODE_AFFINITY_OPERATOR</td><td align="center">true</td><td align="center">“”</td><td align="center">node亲和操作符</td></tr><tr><td align="center">NODE_AFFINITY_VALUES</td><td align="center">true</td><td align="center">“”</td><td align="center">node亲和value</td></tr><tr><td align="center">SPRING_BOOT_ENV</td><td align="center">true</td><td align="center">“”</td><td align="center">全局的环境变量，会追加到每个spring boot的每个pod中，格式 k=v;k1=v2</td></tr></tbody></table><h1 id="Github仓库"><a href="#Github仓库" class="headerlink" title="Github仓库"></a>Github仓库</h1><p>SpringBootOperator: <a href="https://github.com/goudai/spring-boot-operator" target="_blank" rel="noopener">https://github.com/goudai/spring-boot-operator</a></p><p>作者：青木</p>]]></content>
    
    
    
    <tags>
      
      <tag>springboot,k8s,docker</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>手把手从零搭建与运营生产级的 Kubernetes 集群与 KubeSphere</title>
    <link href="/2020/05/28/%E6%89%8B%E6%8A%8A%E6%89%8B%E4%BB%8E%E9%9B%B6%E6%90%AD%E5%BB%BA%E4%B8%8E%E8%BF%90%E8%90%A5%E7%94%9F%E4%BA%A7%E7%BA%A7%E7%9A%84-Kubernetes-%E9%9B%86%E7%BE%A4%E4%B8%8E-KubeSphere/"/>
    <url>/2020/05/28/%E6%89%8B%E6%8A%8A%E6%89%8B%E4%BB%8E%E9%9B%B6%E6%90%AD%E5%BB%BA%E4%B8%8E%E8%BF%90%E8%90%A5%E7%94%9F%E4%BA%A7%E7%BA%A7%E7%9A%84-Kubernetes-%E9%9B%86%E7%BE%A4%E4%B8%8E-KubeSphere/</url>
    
    <content type="html"><![CDATA[<p><img src="D:%5CBlog%5Cyyuangeek.github.io%5Csource_posts%5C%E6%89%8B%E6%8A%8A%E6%89%8B%E4%BB%8E%E9%9B%B6%E6%90%AD%E5%BB%BA%E4%B8%8E%E8%BF%90%E8%90%A5%E7%94%9F%E4%BA%A7%E7%BA%A7%E7%9A%84-Kubernetes-%E9%9B%86%E7%BE%A4%E4%B8%8E-KubeSphere%5C20200327191533.png" srcset="/img/loading.gif" alt="20200327191533"></p><p>本文将从零开始，在干净的机器上安装 Docker、Kubernetes (使用 kubeadm)、Calico、Helm、NFS StorageClass，通过手把手的教程演示如何搭建一个高可用生产级的 Kubernetes，并在 Kubernetes 集群之上安装开源的 <a href="https://github.com/kubesphere/kubesphere" target="_blank" rel="noopener">KubeSphere 容器平台</a>可视化运营集群环境。</p><h2 id="一、准备环境"><a href="#一、准备环境" class="headerlink" title="一、准备环境"></a>一、准备环境</h2><p>开始部署之前，请先确定当前满足如下条件，本次集群搭建，所有机器处于同一内网网段，并且可以互相通信。</p><p>⚠️⚠️⚠️：请详细阅读第一部分，后面的所有操作都是基于这个环境的，为了避免后面部署集群出现各种各样的问题，强烈建议你完全满足第一部分的环境要求</p><blockquote><ul><li>两台以上主机</li><li>每台主机的主机名、Mac 地址、UUID 不相同</li><li>CentOS 7（本文用 7.6/7.7）</li><li>每台机器最好有 2G 内存或以上</li><li>Control-plane/Master至少 2U 或以上</li><li>各个主机之间网络相通</li><li>禁用交换分区</li><li>禁用 SELINUX</li><li>关闭防火墙（我自己的选择，你也可以设置相关防火墙规则）</li><li>Control-plane/Master和Worker节点分别开放如下端口</li></ul></blockquote><p>Master节点</p><table><thead><tr><th align="center">协议</th><th align="center">方向</th><th align="center">端口范围</th><th align="center">作用</th><th align="center">使用者</th></tr></thead><tbody><tr><td align="center">TCP</td><td align="center">入站</td><td align="center">6443*</td><td align="center">Kubernetes API 服务器</td><td align="center">所有组件</td></tr><tr><td align="center">TCP</td><td align="center">入站</td><td align="center">2379-2380</td><td align="center">etcd server client API</td><td align="center">kube-apiserver, etcd</td></tr><tr><td align="center">TCP</td><td align="center">入站</td><td align="center">10250</td><td align="center">Kubelet API</td><td align="center">kubelet 自身、控制平面组件</td></tr><tr><td align="center">TCP</td><td align="center">入站</td><td align="center">10251</td><td align="center">kube-scheduler</td><td align="center">kube-scheduler 自身</td></tr><tr><td align="center">TCP</td><td align="center">入站</td><td align="center">10252</td><td align="center">kube-controller-manager</td><td align="center">kube-controller-manager 自身</td></tr></tbody></table><p>Worker节点</p><table><thead><tr><th align="center">协议</th><th align="center">方向</th><th align="center">端口范围</th><th align="center">作用</th><th align="center">使用者</th></tr></thead><tbody><tr><td align="center">TCP</td><td align="center">入站</td><td align="center">10250</td><td align="center">Kubelet API</td><td align="center">kubelet 自身、控制平面组件</td></tr><tr><td align="center">TCP</td><td align="center">入站</td><td align="center">30000-32767</td><td align="center">NodePort 服务**</td><td align="center">所有组件</td></tr></tbody></table><p>其他相关操作如下：</p><blockquote><p>友情提示😊，如果集群过多，可以了解下 ansible，批量管理你的多台机器，方便实用的工具。</p></blockquote><p>先进行防火墙、交换分区设置</p><pre><code class="hljs bash"><span class="hljs-comment"># 为了方便本操作关闭了防火墙，也建议你这样操作</span>systemctl stop firewalldsystemctl <span class="hljs-built_in">disable</span> firewalld<span class="hljs-comment"># 关闭 SeLinux</span>setenforce 0sed -i <span class="hljs-string">"s/SELINUX=enforcing/SELINUX=disabled/g"</span> /etc/selinux/config<span class="hljs-comment"># 关闭 swap</span>swapoff -ayes | cp /etc/fstab /etc/fstab_bakcat /etc/fstab_bak |grep -v swap &gt; /etc/fstab</code></pre><p>更换CentOS YUM源为阿里云yum源</p><pre><code class="hljs bash"><span class="hljs-comment"># 安装wget</span>yum install wget -y<span class="hljs-comment"># 备份</span>mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup<span class="hljs-comment"># 获取阿里云yum源</span>wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo<span class="hljs-comment"># 获取阿里云epel源</span>wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo<span class="hljs-comment"># 清理缓存并创建新的缓存</span>yum clean all &amp;&amp; yum makecache<span class="hljs-comment"># 系统更新</span>yum update -y</code></pre><p>进行时间同步，并确认时间同步成功</p><pre><code class="hljs shell">timedatectltimedatectl set-ntp true</code></pre><blockquote><p>⚠️⚠️⚠️以下操作请严格按照声明的版本进行部署，否则将碰到乱七八糟的问题</p></blockquote><h1 id="二、安装-Docker"><a href="#二、安装-Docker" class="headerlink" title="二、安装 Docker"></a>二、安装 Docker</h1><h2 id="2-1、安装-Docker"><a href="#2-1、安装-Docker" class="headerlink" title="2.1、安装 Docker"></a>2.1、安装 Docker</h2><p>您需要在每台机器上安装 Docker，我这里安装的是 docker-ce-19.03.4</p><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 安装 Docker CE</span><span class="hljs-meta">#</span><span class="bash"> 设置仓库</span><span class="hljs-meta">#</span><span class="bash"> 安装所需包</span>yum install -y yum-utils \    device-mapper-persistent-data \    lvm2<span class="hljs-meta">#</span><span class="bash"> 新增 Docker 仓库,速度慢的可以换阿里云的源。</span>yum-config-manager \    --add-repo \    https://download.docker.com/linux/centos/docker-ce.repo<span class="hljs-meta">#</span><span class="bash"> 阿里云源地址</span><span class="hljs-meta">#</span><span class="bash"> http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo</span><span class="hljs-meta">#</span><span class="bash"> 安装 Docker CE.</span>yum install -y containerd.io-1.2.10 \    docker-ce-19.03.4 \    docker-ce-cli-19.03.4<span class="hljs-meta">#</span><span class="bash"> 启动 Docker 并添加开机启动</span>systemctl start dockersystemctl enable docker</code></pre><h2 id="2-2、修改-Cgroup-Driver"><a href="#2-2、修改-Cgroup-Driver" class="headerlink" title="2.2、修改 Cgroup Driver"></a>2.2、修改 Cgroup Driver</h2><p>需要将Docker 的 Cgroup Driver 修改为 systemd，不然在为Kubernetes 集群添加节点时会报如下错误：</p><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 执行 kubeadm join 的 WARNING 信息</span>[WARNING IsDockerSystemdCheck]: detected "cgroupfs" as the Docker cgroup driver. The recommended driver is "systemd". Please follow the guide at https://kubernetes.io/docs/setup/cri/</code></pre><p>目前 Docker 的 Cgroup Driver 看起来应该是这样的：</p><pre><code class="hljs java">$ docker info|grep <span class="hljs-string">"Cgroup Driver"</span>  Cgroup Driver: cgroupfs</code></pre><p>需要将这个值修改为 systemd ，同时我将registry替换成国内的一些仓库地址，以免直接在官方仓库拉取镜像会很慢，操作如下。</p><blockquote><p>⚠️⚠️⚠️：注意缩进，直接复制的缩进可能有问题，请确保缩进为正确的 Json 格式；如果 Docker 重启后查看状态不正常，大概率是此文件缩进有问题，Json格式的缩进自己了解一下。</p></blockquote><pre><code class="hljs java"># Setup daemon.cat &gt; /etc/docker/daemon.json &lt;&lt;EOF&#123;    <span class="hljs-string">"exec-opts"</span>: [<span class="hljs-string">"native.cgroupdriver=systemd"</span>],    <span class="hljs-string">"log-driver"</span>: <span class="hljs-string">"json-file"</span>,    <span class="hljs-string">"log-opts"</span>: &#123;    <span class="hljs-string">"max-size"</span>: <span class="hljs-string">"100m"</span>    &#125;,    <span class="hljs-string">"storage-driver"</span>: <span class="hljs-string">"overlay2"</span>,    <span class="hljs-string">"registry-mirrors"</span>:[        <span class="hljs-string">"https://kfwkfulq.mirror.aliyuncs.com"</span>,        <span class="hljs-string">"https://2lqq34jg.mirror.aliyuncs.com"</span>,        <span class="hljs-string">"https://pee6w651.mirror.aliyuncs.com"</span>,        <span class="hljs-string">"http://hub-mirror.c.163.com"</span>,        <span class="hljs-string">"https://docker.mirrors.ustc.edu.cn"</span>,        <span class="hljs-string">"https://registry.docker-cn.com"</span>    ]&#125;EOFmkdir -p /etc/systemd/system/docker.service.d# Restart docker.systemctl daemon-reloadsystemctl restart docker</code></pre><h1 id="三、安装-kubeadm、kubelet-和-kubectl"><a href="#三、安装-kubeadm、kubelet-和-kubectl" class="headerlink" title="三、安装 kubeadm、kubelet 和 kubectl"></a>三、安装 kubeadm、kubelet 和 kubectl</h1><h2 id="3-1、安装准备"><a href="#3-1、安装准备" class="headerlink" title="3.1、安装准备"></a>3.1、安装准备</h2><p>需要在每台机器上安装以下的软件包：</p><ul><li>kubeadm：用来初始化集群的指令。</li><li>kubelet：在集群中的每个节点上用来启动 pod 和容器等。</li><li>kubectl：用来与集群通信的命令行工具（Worker 节点可以不装，但是我装了，不影响什么）。</li></ul><pre><code class="hljs java"># 配置K8S的yum源# 这部分用是阿里云的源，如果可以访问Google，则建议用官方的源cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=http:<span class="hljs-comment">//mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64</span>enabled=<span class="hljs-number">1</span>gpgcheck=<span class="hljs-number">1</span>repo_gpgcheck=<span class="hljs-number">1</span>gpgkey=http:<span class="hljs-comment">//mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg       http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg</span>EOF# 官方源配置如下cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https:<span class="hljs-comment">//packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64</span>enabled=<span class="hljs-number">1</span>gpgcheck=<span class="hljs-number">1</span>repo_gpgcheck=<span class="hljs-number">1</span>gpgkey=https:<span class="hljs-comment">//packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg</span>EOF</code></pre><h2 id="3-2、开始安装"><a href="#3-2、开始安装" class="headerlink" title="3.2、开始安装"></a>3.2、开始安装</h2><p>安装指定版本 kubelet、 kubeadm 、kubectl， 我这里选择当前较新的稳定版 Kubernetes 1.17.3，如果选择的版本不一样，在执行集群初始化的时候，注意 –kubernetes-version 的值。</p><pre><code class="hljs java"># 增加配置cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.confnet.ipv4.ip_forward=<span class="hljs-number">1</span>net.bridge.bridge-nf-call-ip6tables = <span class="hljs-number">1</span>net.bridge.bridge-nf-call-iptables = <span class="hljs-number">1</span>EOF# 加载sysctl --system# 安装yum install -y kubelet-<span class="hljs-number">1.17</span><span class="hljs-number">.3</span> kubeadm-<span class="hljs-number">1.17</span><span class="hljs-number">.3</span> kubectl-<span class="hljs-number">1.17</span><span class="hljs-number">.3</span> --disableexcludes=kubernetes# 启动并设置 kubelet 开机启动systemctl start kubeletsystemctl enable --now kubelet</code></pre><blockquote><p>⚠️⚠️⚠️WARNING</p><p>如果此时执行 systemctl status kubelet 命令，系统日志将得到 kubelet 启动失败的错误提示，请忽略此错误，因为必须完成后续步骤中 kubeadm init 的操作，kubelet 才能正常启动</p></blockquote><h1 id="四、使用-Kubeadm-创建集群"><a href="#四、使用-Kubeadm-创建集群" class="headerlink" title="四、使用 Kubeadm 创建集群"></a>四、使用 Kubeadm 创建集群</h1><h2 id="4-1、初始化-Control-plane-Master-节点"><a href="#4-1、初始化-Control-plane-Master-节点" class="headerlink" title="4.1、初始化 Control-plane/Master 节点"></a>4.1、初始化 Control-plane/Master 节点</h2><p>在第一台 Master 上执行初始化，执行初始化使用 kubeadm init 命令。初始化首先会执行一系列的运行前检查来确保机器满足运行 Kubernetes 的条件，这些检查会抛出警告并在发现错误的时候终止整个初始化进程。 然后 kubeadm init 会下载并安装集群的 Control-plane 组件。</p><p>在初始化之前，需要先设置一下 hosts 解析，为了避免可能出现的问题，后面的 Worker 节点我也进行了同样的操作。注意按照你的实际情况修改Master节点的IP，并且注意 APISERVER_NAME 的值，如果你将这个 apiserver 名称设置为别的值，下面初始化时候的 –control-plane-endpoint 的值保持一致。</p><blockquote><p>提示：为了使 Kubernetes 集群高可用，建议给集群的控制节点配置负载均衡器，如 HAproxy + Keepalived 或 Nginx，云上可以使用公有云的负载均衡器，然后在以下部分设置 MASTER_IP 和 APISERVER_NAME 为负载均衡器的地址（IP:6443） 和域名。</p></blockquote><pre><code class="hljs nginx"><span class="hljs-comment"># 设置hosts</span><span class="hljs-attribute">echo</span> <span class="hljs-string">"127.0.0.1 $(hostname)"</span> &gt;&gt; /etc/hostsexport MASTER_IP=<span class="hljs-number">192.168.115.49</span>export APISERVER_NAME=kuber4s.apiecho <span class="hljs-string">"<span class="hljs-variable">$&#123;MASTER_IP&#125;</span> <span class="hljs-variable">$&#123;APISERVER_NAME&#125;</span>"</span> &gt;&gt; /etc/hosts</code></pre><blockquote><p>友情提示🙂🙂🙂：</p><p>截止2020年01月29日，官方文档声明了使用 kubeadm 初始化 master 时，–config 这个参数是实验性质的，所以就不用了；我们用其他参数一样可以完成 master 的初始化。</p></blockquote><pre><code class="hljs yaml"><span class="hljs-string">--config</span> <span class="hljs-string">string</span>   <span class="hljs-string">kubeadm</span> <span class="hljs-string">配置文件。</span> <span class="hljs-string">警告：配置文件的使用是试验性的。</span></code></pre><p>下面有不带注释的初始化命令，建议先查看带注释的每个参数对应的意义，确保与你的当前配置的环境是一致的，然后再执行初始化操作，避免踩雷。</p><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 初始化 Control-plane/Master 节点</span>kubeadm init \    --apiserver-advertise-address 0.0.0.0 \    # API 服务器所公布的其正在监听的 IP 地址,指定“0.0.0.0”以使用默认网络接口的地址    # 切记只可以是内网IP，不能是外网IP，如果有多网卡，可以使用此选项指定某个网卡    --apiserver-bind-port 6443 \    # API 服务器绑定的端口,默认 6443    --cert-dir /etc/kubernetes/pki \    # 保存和存储证书的路径，默认值："/etc/kubernetes/pki"    --control-plane-endpoint kuber4s.api \    # 为控制平面指定一个稳定的 IP 地址或 DNS 名称,    # 这里指定的 kuber4s.api 已经在 /etc/hosts 配置解析为本机IP    --image-repository registry.cn-hangzhou.aliyuncs.com/google_containers \    # 选择用于拉取Control-plane的镜像的容器仓库，默认值："k8s.gcr.io"    # 因 Google被墙，这里选择国内仓库    --kubernetes-version 1.17.3 \    # 为Control-plane选择一个特定的 Kubernetes 版本， 默认值："stable-1"    --node-name master01 \    #  指定节点的名称,不指定的话为主机hostname，默认可以不指定    --pod-network-cidr 10.10.0.0/16 \    # 指定pod的IP地址范围    --service-cidr 10.20.0.0/16 \    # 指定Service的VIP地址范围    --service-dns-domain cluster.local \    # 为Service另外指定域名，默认"cluster.local"    --upload-certs    # 将 Control-plane 证书上传到 kubeadm-certs Secret</code></pre><p>不带注释的内容如下，如果初始化超时，可以修改DNS为8.8.8.8后重启网络服务再次尝试。</p><pre><code class="hljs shell">kubeadm init \ --apiserver-advertise-address 0.0.0.0 \ --apiserver-bind-port 6443 \ --cert-dir /etc/kubernetes/pki \ --control-plane-endpoint kuber4s.api \ --image-repository registry.cn-hangzhou.aliyuncs.com/google_containers \ --kubernetes-version 1.17.3 \ --pod-network-cidr 10.10.0.0/16 \ --service-cidr 10.20.0.0/16 \ --service-dns-domain cluster.local \ --upload-certs</code></pre><p>接下来这个过程有点漫长（初始化会下载镜像、创建配置文件、启动容器等操作），泡杯茶，耐心等待，你也可以执行 tailf /var/log/messages 来实时查看系统日志，观察 Master 的初始化进展，期间碰到一些报错不要紧张，可能只是暂时的错误，等待最终反馈的结果即可。</p><p>如果初始化最终成功执行，你将看到如下信息：</p><pre><code class="hljs bash">Your Kubernetes control-plane has initialized successfully!To start using your cluster, you need to run the following as a regular user:  mkdir -p <span class="hljs-variable">$HOME</span>/.kube  sudo cp -i /etc/kubernetes/admin.conf <span class="hljs-variable">$HOME</span>/.kube/config  sudo chown $(id -u):$(id -g) <span class="hljs-variable">$HOME</span>/.kube/configYou should now deploy a pod network to the cluster.Run <span class="hljs-string">"kubectl apply -f [podnetwork].yaml"</span> with one of the options listed at:  https://kubernetes.io/docs/concepts/cluster-administration/addons/You can now join any number of the control-plane node running the following <span class="hljs-built_in">command</span> on each as root:  kubeadm join kuber4s.api:6443 --token 0j287q.jw9zfjxud8w85tis \    --discovery-token-ca-cert-hash sha256:5e8bcad5ec97c1025e8044f4b8fd0a4514ecda4bac2b3944f7f39ccae9e4921f \    --control-plane --certificate-key 528b0b9f2861f8f02dfd4a59fc54ad21e42a7dea4dc5552ac24d9c650c5d4d80Please note that the certificate-key gives access to cluster sensitive data, keep it secret!As a safeguard, uploaded-certs will be deleted <span class="hljs-keyword">in</span> two hours; If necessary, you can use<span class="hljs-string">"kubeadm init phase upload-certs --upload-certs"</span> to reload certs afterward.Then you can join any number of worker nodes by running the following on each as root:kubeadm join kuber4s.api:6443 --token 0j287q.jw9zfjxud8w85tis \    --discovery-token-ca-cert-hash sha256:5e8bcad5ec97c1025e8044f4b8fd0a4514ecda4bac2b3944f7f39ccae9e4921f</code></pre><p>为普通用户添加 kubectl 运行权限，命令内容在初始化成功后的输出内容中可以看到。</p><pre><code class="hljs shell">mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config</code></pre><p>建议root用户也进行以上操作，作者使用的是root用户执行的初始化操作，然后在操作完成后查看集群状态的时候，出现如下错误：</p><pre><code class="hljs shell">The connection to the server localhost:8080 was refused - did you specify the right host or port?</code></pre><p>这时候请备份好 kubeadm init 输出中的 kubeadm join 命令，因为将会需要这个命令来给集群添加节点。</p><blockquote><p>⚠️⚠️⚠️提示：令牌是主节点和新添加的节点之间进行相互身份验证的，因此请确保其安全。任何人只要知道了这些令牌，就可以随便给您的集群添加节点。 你可以使用 kubeadm token 命令来查看、创建和删除这类令牌。</p></blockquote><h2 id="4-2、安装-Pod-网络附加组件"><a href="#4-2、安装-Pod-网络附加组件" class="headerlink" title="4.2、安装 Pod 网络附加组件"></a>4.2、安装 Pod 网络附加组件</h2><p>关于 Kubernetes 网络，建议读完这篇 <a href="https://yuerblog.cc/2019/02/25/flannel-and-calico/" target="_blank" rel="noopener">文章</a>，以及文末的其他链接，如<a href="https://juejin.im/entry/599d33ad6fb9a0247804d430" target="_blank" rel="noopener">这个</a>。</p><p>集群必须安装Pod网络插件，以使Pod可以相互通信，只需要在Master节点操作，其他新加入的节点会自动创建相关pod。</p><p>必须在任何应用程序之前部署网络组件。另外，在安装网络之前，CoreDNS将不会启动（你可以通过命令 kubectl get pods –all-namespaces|grep coredns 查看 CoreDNS 的状态）。</p><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 查看 CoreDNS 的状态,并不是 Running 状态</span><span class="hljs-meta">$</span><span class="bash"> kubectl get pods --all-namespaces|grep coredns</span>kube-system   coredns-7f9c544f75-bzksd    0/1   Pending   0     14mkube-system   coredns-7f9c544f75-mtrwq    0/1   Pending   0     14m</code></pre><p>kubeadm 支持多种网络插件，我们选择 Calico 网络插件（kubeadm 仅支持基于容器网络接口（CNI）的网络（不支持kubenet）。），默认情况下，它给出的pod的IP段地址是 192.168.0.0/16 ,如果你的机器已经使用了此IP段，就需要修改这个配置项，将其值改为在初始化 Master 节点时使用 kubeadm init –pod-network-cidr=x.x.x.x/x 的IP地址段，即我们上面配置的 10.10.0.0/16 ，大概在625行左右，操作如下:</p><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 获取配置文件</span>mkdir calico &amp;&amp; cd calicowget https://docs.projectcalico.org/v3.8/manifests/calico.yaml<span class="hljs-meta">#</span><span class="bash"> 修改配置文件</span><span class="hljs-meta">#</span><span class="bash"> 找到 625 行左右的 192.168.0.0/16 ，并修改为我们初始化时配置的 10.10.0.0/16</span>vim calico.yaml<span class="hljs-meta">#</span><span class="bash"> 部署 Pod 网络组件</span>kubectl apply -f calico.yaml</code></pre><p>稍等片刻查询 pod 详情，你也可以使用 watch 命令来实时查看 pod 的状态，等待 Pod 网络组件部署成功后，就可以看到一些信息了，包括 Pod 的 IP 地址信息，这个过程时间可能会有点长。</p><pre><code class="hljs shell">watch -n 2 kubectl get pods --all-namespaces -o wide</code></pre><h2 id="4-3、将-Worker-节点添加到-Kubernetes"><a href="#4-3、将-Worker-节点添加到-Kubernetes" class="headerlink" title="4.3、将 Worker 节点添加到 Kubernetes"></a>4.3、将 Worker 节点添加到 Kubernetes</h2><p>请首先确认 Worker 节点满足第一部分的环境说明，并且已经安装了 Docker 和 kubeadm、kubelet 、kubectl，并且已经启动 kubelet。</p><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 添加 Hosts 解析</span>echo "127.0.0.1 $(hostname)" &gt;&gt; /etc/hostsexport MASTER_IP=192.168.115.49export APISERVER_NAME=kuber4s.apiecho "$&#123;MASTER_IP&#125; $&#123;APISERVER_NAME&#125;" &gt;&gt; /etc/hosts</code></pre><p>将 Worker 节点添加到集群，这里注意，执行后可能会报错，有幸的话你会跳进这个坑，这是因为 Worker 节点加入集群的命令实际上在初始化 master 时已经有提示出来了，不过两小时后会删除上传的证书，所以如果你此时加入集群的时候提示证书相关的错误，请执行 kubeadm init phase upload-certs –upload-certs 重新加载证书。</p><pre><code class="hljs bash">kubeadm join kuber4s.api:6443 --token 0y1dj2.ih27ainxwyib0911 \    --discovery-token-ca-cert-hash sha256:5204b3e358a0d568e147908cba8036bdb63e604d4f4c1c3730398f33144fac61 \</code></pre><p>执行加入操作，你可能会发现卡着不动，大概率是因为令牌ID对此集群无效或已过 2 小时的有效期（通过执行 kubeadm join –v=5 来获取详细的加入过程，看到了内容为 ”token id “0y1dj2” is invalid for this cluster or it has expired“ 的提示），接下来需要在 Master 上通过 kubeadm token create 来创建新的令牌。</p><pre><code class="hljs sh">$ kubeadm token create --<span class="hljs-built_in">print</span>-join-commandW0129 19:10:04.842735   15533 validation.go:28] Cannot validate kube-proxy config - no validator is availableW0129 19:10:04.842808   15533 validation.go:28] Cannot validate kubelet config - no validator is available<span class="hljs-comment"># 输出结果如下</span>kubeadm join kuber4s.api:6443 --token 1hk9bc.oz7f3lmtbzf15x9b     --discovery-token-ca-cert-hash sha256:5e8bcad5ec97c1025e8044f4b8fd0a4514ecda4bac2b3944f7f39ccae9e4921f</code></pre><p>在 Worker 节点上重新执行加入集群命令</p><pre><code class="hljs shell">kubeadm join kuber4s.api:6443 \    --token 1hk9bc.oz7f3lmtbzf15x9b \    --discovery-token-ca-cert-hash sha256:5e8bcad5ec97c1025e8044f4b8fd0a4514ecda4bac2b3944f7f39ccae9e4921f</code></pre><p>接下来在Master上查看 Worker 节点加入的状况，直到 Worker 节点的状态变为 Ready 便证明加入成功，这个过程可能会有点漫长，30 分钟以内都算正常的，主要看你网络的情况或者说拉取镜像的速度；另外不要一看到 /var/log/messages 里面报错就慌了，那也得看具体报什么错，看不懂就稍微等一下，一般在 Master 上能看到已经加入（虽然没有Ready）就没什么问题。</p><pre><code class="hljs shell">watch kubectl get nodes -o wide</code></pre><h2 id="4-4、添加-Master-节点"><a href="#4-4、添加-Master-节点" class="headerlink" title="4.4、添加 Master 节点"></a>4.4、添加 Master 节点</h2><p>需要至少2个CPU核心，否则会报错</p><pre><code class="hljs shell">kubeadm join kuber4s.api:6443 \    --token 1hk9bc.oz7f3lmtbzf15x9b \    --discovery-token-ca-cert-hash sha256:5e8bcad5ec97c1025e8044f4b8fd0a4514ecda4bac2b3944f7f39ccae9e4921f \    --control-plane --certificate-key 5253fc7e9a4e6204d0683ed2d60db336b3ff64ddad30ba59b4c0bf40d8ccadcd</code></pre><h2 id="4-5、补充内容"><a href="#4-5、补充内容" class="headerlink" title="4.5、补充内容"></a>4.5、补充内容</h2><ul><li><a href="https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-init/" target="_blank" rel="noopener">kubeadm init</a> 初始化 Kubernetes 主节点</li><li><a href="https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-token/" target="_blank" rel="noopener">kubeadm token</a> 管理 kubeadm join 的令牌</li><li><a href="https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-reset/" target="_blank" rel="noopener">kubeadm reset</a> 将 kubeadm init 或 kubeadm join 对主机的更改恢复到之前状态，一般与 -f 参数使用</li></ul><p>移除 worker 节点</p><p>正常情况下，你无需移除 worker 节点，如果要移除，在准备移除的 worker 节点上执行</p><pre><code class="hljs shell">kubeadm reset -f</code></pre><p>或者在 Control-plane 上执行</p><pre><code class="hljs shell">kubectl delete node nodename</code></pre><blockquote><ul><li>将 nodename 替换为要移除的 worker 节点的名字</li><li>worker 节点的名字可以通过在 Control-plane 上执行 kubectl get nodes 命令获得</li></ul></blockquote><h1 id="五、Kubernetes-高可用集群"><a href="#五、Kubernetes-高可用集群" class="headerlink" title="五、Kubernetes 高可用集群"></a>五、Kubernetes 高可用集群</h1><h2 id="5-1、环境说明"><a href="#5-1、环境说明" class="headerlink" title="5.1、环境说明"></a>5.1、环境说明</h2><p>如果你使用的是以上方法部署你的 Kubernetes 集群，想在当前基础上进行高可用集群的创建，则可以按照下面的步骤继续进行。</p><p>值得注意的是，这里没有将ETCD放在Master外的机器上，而是使用默认的架构，即官方的 Stacked etcd topology 方式的集群</p><p><img src="D:%5CBlog%5Cyyuangeek.github.io%5Csource_posts%5C%E6%89%8B%E6%8A%8A%E6%89%8B%E4%BB%8E%E9%9B%B6%E6%90%AD%E5%BB%BA%E4%B8%8E%E8%BF%90%E8%90%A5%E7%94%9F%E4%BA%A7%E7%BA%A7%E7%9A%84-Kubernetes-%E9%9B%86%E7%BE%A4%E4%B8%8E-KubeSphere%5C2020-03-25-091655.png" srcset="/img/loading.gif" alt="2020-03-25-091655"></p><p>你需要至少 3 台 Master 节点和 3 台 Worker 节点，或者更多的机器，但要保证是 Master 和 Worker 节点数都是奇数的，以防止 leader 选举时出现脑裂状况。</p><table><thead><tr><th align="center">机器名称</th><th align="center">机器IP</th><th align="center">工作内容</th></tr></thead><tbody><tr><td align="center">master01</td><td align="center">192.168.115.49</td><td align="center">master、etcd</td></tr><tr><td align="center">master02</td><td align="center">192.168.115.41</td><td align="center">master、etcd</td></tr><tr><td align="center">master03</td><td align="center">192.168.115.42</td><td align="center">master、etcd</td></tr><tr><td align="center">node01</td><td align="center">192.168.115.46</td><td align="center">worker</td></tr><tr><td align="center">node02</td><td align="center">192.168.115.47</td><td align="center">worker</td></tr><tr><td align="center">node03</td><td align="center">192.168.115.48</td><td align="center">worker</td></tr><tr><td align="center">nfs</td><td align="center">192.168.115.50</td><td align="center">存储</td></tr></tbody></table><h2 id="5-2、高可用扩展"><a href="#5-2、高可用扩展" class="headerlink" title="5.2、高可用扩展"></a>5.2、高可用扩展</h2><p>Kubernetes 的高可用扩展其实挺简单，你只需要将不同的 Master 和 Worker 节点加入到集群中就行了。加入的指令在你初始化集群时已经给出了。</p><ul><li>添加 Master 节点：</li></ul><p>需要至少 2 个 CPU 核心，否则会报错</p><pre><code class="hljs bash">kubeadm join kuber4s.api:6443 \ --token 1hk9bc.oz7f3lmtbzf15x9b \ --discovery-token-ca-cert-hash sha256:5e8bcad5ec97c1025e8044f4b8fd0a4514ecda4bac2b3944f7f39ccae9e4921f \ --control-plane --certificate-key 5253fc7e9a4e6204d0683ed2d60db336b3ff64ddad30ba59b4c0bf40d8ccadcd</code></pre><ul><li>添加 Worker 节点</li></ul><p>在 Worker 节点上重新执行加入集群命令</p><pre><code class="hljs bash">kubeadm join kuber4s.api:6443 \--token 1hk9bc.oz7f3lmtbzf15x9b \--discovery-token-ca-cert-hash sha256:5e8bcad5ec97c1025e8044f4b8fd0a4514ecda4bac2b3944f7f39ccae9e4921f</code></pre><h1 id="六、安装-KubeSphere"><a href="#六、安装-KubeSphere" class="headerlink" title="六、安装 KubeSphere"></a>六、安装 KubeSphere</h1><h2 id="6-1、KubeSphere简介"><a href="#6-1、KubeSphere简介" class="headerlink" title="6.1、KubeSphere简介"></a>6.1、KubeSphere简介</h2><p>Kubernetes 官方有提供一套 Dashboard，但是我这里选择功能更强大的 KubeSphere，以下内容引用自 KubeSphere 官网：</p><p><a href="https://kubesphere.com.cn/docs/zh-CN/" target="_blank" rel="noopener">KubeSphere</a> 是在 <a href="https://kubernetes.io/" target="_blank" rel="noopener">Kubernetes</a> 之上构建的以应用为中心的容器平台，提供简单易用的操作界面以及向导式操作方式，在降低用户使用容器调度平台学习成本的同时，极大减轻开发、测试、运维的日常工作的复杂度，旨在解决 Kubernetes 本身存在的存储、网络、安全和易用性等痛点。除此之外，平台已经整合并优化了多个适用于容器场景的功能模块，以完整的解决方案帮助企业轻松应对敏捷开发与自动化运维、DevOps、微服务治理、灰度发布、多租户管理、工作负载和集群管理、监控告警、日志查询与收集、服务与网络、应用商店、镜像构建与镜像仓库管理和存储管理等多种场景。后续版本将提供和支持多集群管理、大数据、AI 等场景。</p><p><img src="D:%5CBlog%5Cyyuangeek.github.io%5Csource_posts%5C%E6%89%8B%E6%8A%8A%E6%89%8B%E4%BB%8E%E9%9B%B6%E6%90%AD%E5%BB%BA%E4%B8%8E%E8%BF%90%E8%90%A5%E7%94%9F%E4%BA%A7%E7%BA%A7%E7%9A%84-Kubernetes-%E9%9B%86%E7%BE%A4%E4%B8%8E-KubeSphere%5C20200327114511.png" srcset="/img/loading.gif" alt="20200327114511"></p><h2 id="6-2、安装要求"><a href="#6-2、安装要求" class="headerlink" title="6.2、安装要求"></a>6.2、安装要求</h2><p>KubeSphere 支持直接在 Linux 上部署集群，也支持在 Kubernetes 上部署，我这里选择后者，基本的要求如下：</p><ul><li>Kubernetes 版本：1.15.x ≤ K8s version ≤ 1.17.x；</li><li>Helm 版本：2.10.0 ≤ Helm Version ＜ 3.0.0（不支持 helm 2.16.0<a href="https://github.com/helm/helm/issues/6894" target="_blank" rel="noopener">#6894</a>），且已安装了 Tiller，参考 <a href="https://devopscube.com/install-configure-helm-kubernetes/" target="_blank" rel="noopener">如何安装与配置 Helm</a>（预计 3.0 支持 Helm v3）；</li><li>集群已有默认的存储类型（StorageClass），若还没有准备存储请参考<a href="https://kubesphere.com.cn/docs/zh-CN/appendix/install-openebs" target="_blank" rel="noopener">安装 OpenEBS 创建 LocalPV 存储类型</a>用作开发测试环境。</li><li>集群能够访问外网，若无外网请参考 <a href="https://kubesphere.com.cn/docs/installation/install-on-k8s-airgapped/" target="_blank" rel="noopener">在 Kubernetes 离线安装 KubeSphere</a>。</li></ul><h2 id="6-3、安装-Helm"><a href="#6-3、安装-Helm" class="headerlink" title="6.3、安装 Helm"></a>6.3、安装 Helm</h2><h3 id="6-3-1、Helm-简介"><a href="#6-3-1、Helm-简介" class="headerlink" title="6.3.1、Helm 简介"></a>6.3.1、Helm 简介</h3><p>Helm 基本思想如图所示</p><p><img src="D:%5CBlog%5Cyyuangeek.github.io%5Csource_posts%5C%E6%89%8B%E6%8A%8A%E6%89%8B%E4%BB%8E%E9%9B%B6%E6%90%AD%E5%BB%BA%E4%B8%8E%E8%BF%90%E8%90%A5%E7%94%9F%E4%BA%A7%E7%BA%A7%E7%9A%84-Kubernetes-%E9%9B%86%E7%BE%A4%E4%B8%8E-KubeSphere%5C2020-03-25-095440.png" srcset="/img/loading.gif" alt="2020-03-25-095440"></p><p>以下内容引用自 <a href="https://blog.csdn.net/weixin_30566063/article/details/99247145" target="_blank" rel="noopener">此篇文章</a></p><p>Helm 基本概念</p><p>Helm 可以理解为 Kubernetes 的包管理工具，可以方便地发现、共享和使用为Kubernetes构建的应用，它包含几个基本概念：</p><ul><li>Chart：一个 Helm 包，其中包含了运行一个应用所需要的镜像、依赖和资源定义等，还可能包含 Kubernetes 集群中的服务定义</li><li>Release: 在 Kubernetes 集群上运行的 Chart 的一个实例。在同一个集群上，一个 Chart 可以安装很多次。每次安装都会创建一个新的 release。例如一个 MySQL Chart，如果想在服务器上运行两个数据库，就可以把这个 Chart 安装两次。每次安装都会生成自己的 Release，会有自己的 Release 名称。</li><li>Repository：用于发布和存储 Chart 的仓库。</li></ul><h3 id="6-3-2、Helm安装"><a href="#6-3-2、Helm安装" class="headerlink" title="6.3.2、Helm安装"></a>6.3.2、Helm安装</h3><p>安装过程如下</p><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 创建部署目录并下载Helm</span>mkdir tillercd tiller<span class="hljs-meta">#</span><span class="bash"> 先使用官方的方式安装，如果安装不了，可以看到下载文件的地址，然后手动下载解压</span>curl -L https://git.io/get_helm.sh | bash<span class="hljs-meta">#</span><span class="bash"> 获取到下载地址后，想办法下载</span>wget https://get.helm.sh/helm-v2.16.3-linux-amd64.tar.gztar zxf helm-v2.16.3-linux-amd64.tar.gzmv linux-amd64/helm /usr/local/bin/helm<span class="hljs-meta">#</span><span class="bash"> 验证</span>helm version</code></pre><p>部署 Tiller，即 Helm 的服务端。先创建 SA</p><pre><code class="hljs yaml"><span class="hljs-comment"># yaml文件如下</span><span class="hljs-string">$</span> <span class="hljs-string">cat</span> <span class="hljs-string">/root/tiller/helm-rbac.yaml</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ServiceAccount</span><span class="hljs-attr">metadata:</span> <span class="hljs-attr">name:</span> <span class="hljs-string">tiller</span> <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span><span class="hljs-meta">---</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRoleBinding</span><span class="hljs-attr">metadata:</span> <span class="hljs-attr">name:</span> <span class="hljs-string">tiller</span><span class="hljs-attr">roleRef:</span> <span class="hljs-attr">apiGroup:</span> <span class="hljs-string">rbac.authorization.k8s.io</span> <span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span> <span class="hljs-attr">name:</span> <span class="hljs-string">cluster-admin</span><span class="hljs-attr">subjects:</span> <span class="hljs-bullet">-</span> <span class="hljs-attr">kind:</span> <span class="hljs-string">ServiceAccount</span> <span class="hljs-attr">name:</span> <span class="hljs-string">tiller</span> <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span></code></pre><p>创建 RBAC：</p><pre><code class="hljs shell">kubectl apply -f helm-rbac.yaml</code></pre><p>初始化，这个过程可能不会成功，具体接着往下看</p><pre><code class="hljs shell">helm init --service-account=tiller --history-max 300</code></pre><p>检查初始化的情况，不出意外的话，墙内用户看pod详情可以看到获取不到镜像的错误。</p><pre><code class="hljs shell">kubectl get deployment tiller-deploy -n kube-system</code></pre><p>如果一直获取不到镜像，可以通过更换到Azure中国镜像源来解决，操作步骤如下：</p><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 编辑 deploy</span>kubectl edit deploy tiller-deploy -n kube-system<span class="hljs-meta">#</span><span class="bash"> 查找到image地址，替换为如下地址，保存退出</span>gcr.azk8s.cn/kubernetes-helm/tiller:v2.16.3</code></pre><p>接下来稍等片刻，再次查看deployment和pod详情，就正常了</p><pre><code class="hljs shell">kubectl get deployment tiller-deploy -n kube-system</code></pre><h2 id="6-4、安装-StorageClass"><a href="#6-4、安装-StorageClass" class="headerlink" title="6.4、安装 StorageClass"></a>6.4、安装 StorageClass</h2><p>Kubernetes 支持多种 StorageClass，我这选择 NFS 作为集群的 StorageClass。</p><p>参考地址：<a href="https://github.com/kubernetes-incubator/external-storage/tree/master/nfs-client" target="_blank" rel="noopener">https://github.com/kubernetes-incubator/external-storage/tree/master/nfs-client</a></p><h3 id="6-4-1、下载所需文件"><a href="#6-4-1、下载所需文件" class="headerlink" title="6.4.1、下载所需文件"></a>6.4.1、下载所需文件</h3><p>下载所需文件，并进行内容调整</p><pre><code class="hljs shell">mkdir nfsvolume &amp;&amp; cd nfsvolumefor file in class.yaml deployment.yaml rbac.yaml ; do wget https://raw.githubusercontent.com/kubernetes-incubator/external-storage/master/nfs-client/deploy/$file ; done</code></pre><p>修改 deployment.yaml 中的两处 NFS 服务器 IP 和目录</p><pre><code class="hljs yaml"><span class="hljs-string">...</span>          <span class="hljs-attr">env:</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">PROVISIONER_NAME</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">fuseim.pri/ifs</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">NFS_SERVER</span>              <span class="hljs-attr">value:</span> <span class="hljs-number">192.168</span><span class="hljs-number">.115</span><span class="hljs-number">.50</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">NFS_PATH</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">/data/k8s</span>      <span class="hljs-attr">volumes:</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">nfs-client-root</span>          <span class="hljs-attr">nfs:</span>            <span class="hljs-attr">server:</span> <span class="hljs-number">192.168</span><span class="hljs-number">.115</span><span class="hljs-number">.50</span>            <span class="hljs-attr">path:</span> <span class="hljs-string">/data/k8s</span></code></pre><h2 id="6-4-2、部署创建"><a href="#6-4-2、部署创建" class="headerlink" title="6.4.2、部署创建"></a>6.4.2、部署创建</h2><p>具体的说明可以去官网查看。</p><pre><code class="hljs shell">kubectl create -f rbac.yamlkubectl create -f class.yamlkubectl create -f deployment.yaml</code></pre><p>如果日志中看到“上有坏超级块”，请在集群内所有机器上安装nfs-utils并启动。</p><pre><code class="hljs shell">yum -y install nfs-utilssystemctl start nfs-utilssystemctl enable nfs-utilsrpcinfo -p</code></pre><p>查看storageclass</p><pre><code class="hljs java">$ kubectl get storageclassNAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGEmanaged-nfs-storage fuseim.pri/ifs Delete Immediate <span class="hljs-keyword">false</span> <span class="hljs-number">10</span>m</code></pre><h3 id="6-4-3、标记一个默认的-StorageClass"><a href="#6-4-3、标记一个默认的-StorageClass" class="headerlink" title="6.4.3、标记一个默认的 StorageClass"></a>6.4.3、标记一个默认的 StorageClass</h3><p>操作命令格式如下</p><pre><code class="hljs shell">kubectl patch storageclass  -p '&#123;"metadata": &#123;"annotations":&#123;"storageclass.kubernetes.io/is-default-class":"true"&#125;&#125;&#125;'</code></pre><p>请注意，最多只能有一个 StorageClass 能够被标记为默认。</p><p>验证标记是否成功</p><pre><code class="hljs shell"><span class="hljs-meta">$</span><span class="bash"> kubectl get storageclass</span>NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGEmanaged-nfs-storage (default) fuseim.pri/ifs Delete Immediate false 12m</code></pre><h2 id="6-5、部署-KubeSphere"><a href="#6-5、部署-KubeSphere" class="headerlink" title="6.5、部署 KubeSphere"></a>6.5、部署 KubeSphere</h2><p>过程很简单，如果你的机器资源足够，建议你进行完整安装，操作步骤如下。如果你的资源不是很充足，则可以进行最小化安装，<a href="https://kubesphere.com.cn/docs/zh-CN/installation/prerequisites/" target="_blank" rel="noopener">参考地址</a>。我当然是选择完整安装了，香！</p><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 下载 yaml 文件</span>mkdir kubesphere &amp;&amp; cd kubespherewget https://raw.githubusercontent.com/kubesphere/ks-installer/master/kubesphere-complete-setup.yaml<span class="hljs-meta">#</span><span class="bash"> 部署 KubeSphere</span>kubectl apply -f kubesphere-complete-setup.yaml</code></pre><p>这个过程根据你实际网速，实际使用时间长度有所不同。你可以通过如下命令查看实时的日志输出。</p><pre><code class="hljs shell">kubectl logs -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-install -o jsonpath='&#123;.items[0].metadata.name&#125;') -f</code></pre><p>当你看到如下日志输出，证明你的 KubeSphere 部署成功</p><pre><code class="hljs bash">**************************************************task monitoring status is successfultask notification status is successfultask devops status is successfultask alerting status is successfultask logging status is successfultask openpitrix status is successfultask servicemesh status is successfultotal: 7     completed:7**************************************************<span class="hljs-comment">#####################################################</span><span class="hljs-comment">###              Welcome to KubeSphere!           ###</span><span class="hljs-comment">#####################################################</span>Console: http://192.168.115.49:30880Account: adminPassword: P@88w0rd<span class="hljs-comment">#####################################################</span></code></pre><p>确认 Pod 都正常运行后，可使用IP:30880访问 KubeSphere UI 界面，默认的集群管理员账号为admin/P@88w0rd，Enjoy it，😏！</p><p><img src="D:%5CBlog%5Cyyuangeek.github.io%5Csource_posts%5C%E6%89%8B%E6%8A%8A%E6%89%8B%E4%BB%8E%E9%9B%B6%E6%90%AD%E5%BB%BA%E4%B8%8E%E8%BF%90%E8%90%A5%E7%94%9F%E4%BA%A7%E7%BA%A7%E7%9A%84-Kubernetes-%E9%9B%86%E7%BE%A4%E4%B8%8E-KubeSphere%5C2020-03-25-103029.png" srcset="/img/loading.gif" alt="2020-03-25-103029"></p><p><img src="D:%5CBlog%5Cyyuangeek.github.io%5Csource_posts%5C%E6%89%8B%E6%8A%8A%E6%89%8B%E4%BB%8E%E9%9B%B6%E6%90%AD%E5%BB%BA%E4%B8%8E%E8%BF%90%E8%90%A5%E7%94%9F%E4%BA%A7%E7%BA%A7%E7%9A%84-Kubernetes-%E9%9B%86%E7%BE%A4%E4%B8%8E-KubeSphere%5C2020-03-25-145648.png" srcset="/img/loading.gif" alt="2020-03-25-145648"></p><p>作者：KubeSphere</p>]]></content>
    
    
    
    <tags>
      
      <tag>k8s,cloudnative,docker</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>在Kubernetes上部署Elasticsearch集群</title>
    <link href="/2020/05/28/%E5%9C%A8Kubernetes%E4%B8%8A%E9%83%A8%E7%BD%B2Elasticsearch%E9%9B%86%E7%BE%A4/"/>
    <url>/2020/05/28/%E5%9C%A8Kubernetes%E4%B8%8A%E9%83%A8%E7%BD%B2Elasticsearch%E9%9B%86%E7%BE%A4/</url>
    
    <content type="html"><![CDATA[<blockquote><p>尝试在上一篇文章中搭建的K8S集群上部署ES集群，去年年中的时候，未来搭建ELK，学习过一段时间的ES，在虚拟机里搭建过简单的集群环境。现在K8S里再次搭建的时候，发现当时学过的很多概念都生疏了，又找到之前的学习记录复习了一遍——好记忆不如烂笔头。言归正传</p></blockquote><h2 id="1、环境清单"><a href="#1、环境清单" class="headerlink" title="1、环境清单"></a>1、环境清单</h2><h3 id="1-1、系统清单"><a href="#1-1、系统清单" class="headerlink" title="1.1、系统清单"></a>1.1、系统清单</h3><p>IP                              Hostname    Role          OS<br>192.168.119.160    k8s-master    Master    CentOS 7<br>192.168.119.161    k8s-node-1    Node       CentOS 7<br>192.168.119.162    k8s-node-2    Node        CentOS 7<br>192.168.119.163    k8s-node-3    Node        CentOS 7</p><blockquote><p>在之前的基础上多加了一个node节点，因为在两个node的时候部署ES集群，内存不够，然后一直提示提示OPENJDK GC Kill之类的信息。</p></blockquote><h3 id="1-2、镜像清单"><a href="#1-2、镜像清单" class="headerlink" title="1.2、镜像清单"></a>1.2、镜像清单</h3><ul><li><a href="http://docker.elastic.co/elasticsearch/elasticsearch:6.2.2" target="_blank" rel="noopener">docker.elastic.co/elasticsearch/elasticsearch:6.2.2</a></li></ul><blockquote><p>下载地址参考：<a href="https://www.docker.elastic.co/" target="_blank" rel="noopener">https://www.docker.elastic.co</a>，下文附网盘下载链接。</p></blockquote><h2 id="2、部署说明"><a href="#2、部署说明" class="headerlink" title="2、部署说明"></a>2、部署说明</h2><h3 id="2-1、-导入镜像"><a href="#2-1、-导入镜像" class="headerlink" title="2.1、 导入镜像"></a>2.1、 导入镜像</h3><blockquote><p>将镜像导入私库或者你的所有k8s节点上的docker内。</p></blockquote><h3 id="2-2、-编写Dockerfile，修改镜像"><a href="#2-2、-编写Dockerfile，修改镜像" class="headerlink" title="2.2、 编写Dockerfile，修改镜像"></a>2.2、 编写Dockerfile，修改镜像</h3><pre><code class="hljs shell">[root@k8s-master dockerfile]# pwd/root/dockerfile[root@k8s-master dockerfile]# ll-rw-r--r--. 1 root root       156 3月   5 16:28 Dockerfile-rw-r--r--. 1 root root        98 3月   2 15:26 run.sh<span class="hljs-meta">#</span><span class="bash"><span class="hljs-comment">## Dockerfile文件内容 ###</span></span>[root@k8s-master dockerfile]# vi DockerfileFROM docker.elastic.co/elasticsearch/elasticsearch:6.2.2MAINTAINER chenlei leichen.china@gmail.comCOPY run.sh /RUN chmod 775 /run.shCMD ["/run.sh"]<span class="hljs-meta">#</span><span class="bash"><span class="hljs-comment">## 启动脚本内容 ###</span></span>[root@k8s-master dockerfile]# vi run.sh<span class="hljs-meta">#</span><span class="bash">!/bin/bash</span><span class="hljs-meta">#</span><span class="bash"> 设置memlock无限制</span>ulimit -l unlimitedexec su elasticsearch /usr/local/bin/docker-entrypoint.sh<span class="hljs-meta">#</span><span class="bash"><span class="hljs-comment">## 构建镜像 ###</span></span>[root@k8s-master dockerfile]# docker build --tag docker.elastic.co/elasticsearch/elasticsearch:6.2.2-1 .</code></pre><blockquote><p>之所以要修改镜像，是因为在k8s中不能直接修改系统参数（ulimit）。<a href="https://www.elastic.co/guide/en/elasticsearch/reference/6.2/docker.html" target="_blank" rel="noopener">官网上的事例</a>是在docker上部署的，docker可以通过ulimits来修改用户资源限制。但是k8s上各种尝试各种碰壁，<a href="https://github.com/kubernetes/kubernetes/issues/3595" target="_blank" rel="noopener">老外也有不少争论</a>。</p></blockquote><blockquote><p>通过上述方式，<a href="http://xn--run-c88dj4aob82jnvcuxbk0nowbj59bvwfmwqiqlvw3eka6363c9gze.sh/" target="_blank" rel="noopener">将镜像的入口命令修改为我们自己的run.sh</a>，然后在脚本内设置memlock，最后调研原本的启动脚本来达到目的。</p></blockquote><h3 id="2-3、ES集群规划"><a href="#2-3、ES集群规划" class="headerlink" title="2.3、ES集群规划"></a>2.3、ES集群规划</h3><p>Elasticsearch我选用的6.2.2的最新版本，集群节点分饰三种角色：master、data、ingest，每个节点担任至少一种角色。每个觉得均有对应的参数指定，下文将描述。</p><p>标注为master的节点表示master候选节点，具备成为master的资格，实际的master节点在运行时由其他节点投票产生，集群健康的情况下，同一时刻只有一个master节点，否则称之为脑裂，会导致数据不一致。</p><table><thead><tr><th>ES节点（POD）</th><th>是否Master</th><th>是否Data</th><th align="center">是否Ingest</th></tr></thead><tbody><tr><td>节点1</td><td>Y</td><td>N</td><td align="center">N</td></tr><tr><td>节点2</td><td>Y</td><td>N</td><td align="center">N</td></tr><tr><td>节点3</td><td>Y</td><td>N</td><td align="center">N</td></tr><tr><td>节点4</td><td>N</td><td>Y</td><td align="center">Y</td></tr><tr><td>节点5</td><td>N</td><td>Y</td><td align="center">Y</td></tr></tbody></table><h3 id="2-4、编写YML文件并部署集群"><a href="#2-4、编写YML文件并部署集群" class="headerlink" title="2.4、编写YML文件并部署集群"></a>2.4、编写YML文件并部署集群</h3><p>仅此一个文件：elasticsearch-cluster.yml，内容如下：</p><pre><code class="hljs yaml"><span class="hljs-meta">---</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Namespace</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">ns-elasticsearch</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">name:</span> <span class="hljs-string">ns-elasticsearch</span><span class="hljs-meta">---</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ServiceAccount</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">elastic-app:</span> <span class="hljs-string">elasticsearch</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">elasticsearch-admin</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">ns-elasticsearch</span><span class="hljs-meta">---</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRoleBinding</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">elasticsearch-admin</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">elastic-app:</span> <span class="hljs-string">elasticsearch</span><span class="hljs-attr">roleRef:</span>  <span class="hljs-attr">apiGroup:</span> <span class="hljs-string">rbac.authorization.k8s.io</span>  <span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">cluster-admin</span><span class="hljs-attr">subjects:</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">kind:</span> <span class="hljs-string">ServiceAccount</span>    <span class="hljs-attr">name:</span> <span class="hljs-string">elasticsearch-admin</span>    <span class="hljs-attr">namespace:</span> <span class="hljs-string">ns-elasticsearch</span><span class="hljs-meta">---</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Deployment</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">apps/v1beta2</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">elastic-app:</span> <span class="hljs-string">elasticsearch</span>    <span class="hljs-attr">role:</span> <span class="hljs-string">master</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">elasticsearch-master</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">ns-elasticsearch</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">replicas:</span> <span class="hljs-number">3</span>  <span class="hljs-attr">revisionHistoryLimit:</span> <span class="hljs-number">10</span>  <span class="hljs-attr">selector:</span>    <span class="hljs-attr">matchLabels:</span>      <span class="hljs-attr">elastic-app:</span> <span class="hljs-string">elasticsearch</span>      <span class="hljs-attr">role:</span> <span class="hljs-string">master</span>  <span class="hljs-attr">template:</span>    <span class="hljs-attr">metadata:</span>      <span class="hljs-attr">labels:</span>        <span class="hljs-attr">elastic-app:</span> <span class="hljs-string">elasticsearch</span>        <span class="hljs-attr">role:</span> <span class="hljs-string">master</span>    <span class="hljs-attr">spec:</span>      <span class="hljs-attr">containers:</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">elasticsearch-master</span>          <span class="hljs-attr">image:</span> <span class="hljs-string">docker.elastic.co/elasticsearch/elasticsearch:6.2.2-1</span>          <span class="hljs-attr">lifecycle:</span>            <span class="hljs-attr">postStart:</span>              <span class="hljs-attr">exec:</span>                <span class="hljs-attr">command:</span> <span class="hljs-string">["/bin/bash",</span> <span class="hljs-string">"-c"</span><span class="hljs-string">,</span> <span class="hljs-string">"sysctl -w vm.max_map_count=262144; ulimit -l unlimited;"</span><span class="hljs-string">]</span>          <span class="hljs-attr">ports:</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">containerPort:</span> <span class="hljs-number">9200</span>              <span class="hljs-attr">protocol:</span> <span class="hljs-string">TCP</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">containerPort:</span> <span class="hljs-number">9300</span>              <span class="hljs-attr">protocol:</span> <span class="hljs-string">TCP</span>          <span class="hljs-attr">env:</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">"cluster.name"</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">"elasticsearch-cluster"</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">"bootstrap.memory_lock"</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">"true"</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">"discovery.zen.ping.unicast.hosts"</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">"elasticsearch-discovery"</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">"discovery.zen.minimum_master_nodes"</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">"2"</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">"discovery.zen.ping_timeout"</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">"5s"</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">"node.master"</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">"true"</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">"node.data"</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">"false"</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">"node.ingest"</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">"false"</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">"ES_JAVA_OPTS"</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">"-Xms256m -Xmx256m"</span>          <span class="hljs-attr">securityContext:</span>            <span class="hljs-attr">privileged:</span> <span class="hljs-literal">true</span>      <span class="hljs-attr">serviceAccountName:</span> <span class="hljs-string">elasticsearch-admin</span>      <span class="hljs-attr">tolerations:</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">key:</span> <span class="hljs-string">node-role.kubernetes.io/master</span>          <span class="hljs-attr">effect:</span> <span class="hljs-string">NoSchedule</span><span class="hljs-meta">---</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Service</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">elastic-app:</span> <span class="hljs-string">elasticsearch</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">elasticsearch-discovery</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">ns-elasticsearch</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">ports:</span>    <span class="hljs-bullet">-</span> <span class="hljs-attr">port:</span> <span class="hljs-number">9300</span>      <span class="hljs-attr">targetPort:</span> <span class="hljs-number">9300</span>  <span class="hljs-attr">selector:</span>    <span class="hljs-attr">elastic-app:</span> <span class="hljs-string">elasticsearch</span>    <span class="hljs-attr">role:</span> <span class="hljs-string">master</span><span class="hljs-meta">---</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Deployment</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">apps/v1beta2</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">elastic-app:</span> <span class="hljs-string">elasticsearch</span>    <span class="hljs-attr">role:</span> <span class="hljs-string">data</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">elasticsearch-data</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">ns-elasticsearch</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">replicas:</span> <span class="hljs-number">2</span>  <span class="hljs-attr">revisionHistoryLimit:</span> <span class="hljs-number">10</span>  <span class="hljs-attr">selector:</span>    <span class="hljs-attr">matchLabels:</span>      <span class="hljs-attr">elastic-app:</span> <span class="hljs-string">elasticsearch</span>  <span class="hljs-attr">template:</span>    <span class="hljs-attr">metadata:</span>      <span class="hljs-attr">labels:</span>        <span class="hljs-attr">elastic-app:</span> <span class="hljs-string">elasticsearch</span>        <span class="hljs-attr">role:</span> <span class="hljs-string">data</span>    <span class="hljs-attr">spec:</span>      <span class="hljs-attr">containers:</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">elasticsearch-data</span>          <span class="hljs-attr">image:</span> <span class="hljs-string">docker.elastic.co/elasticsearch/elasticsearch:6.2.2-1</span>          <span class="hljs-attr">lifecycle:</span>            <span class="hljs-attr">postStart:</span>              <span class="hljs-attr">exec:</span>                <span class="hljs-attr">command:</span> <span class="hljs-string">["/bin/bash",</span> <span class="hljs-string">"-c"</span><span class="hljs-string">,</span> <span class="hljs-string">"sysctl -w vm.max_map_count=262144; ulimit -l unlimited;"</span><span class="hljs-string">]</span>          <span class="hljs-attr">ports:</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">containerPort:</span> <span class="hljs-number">9200</span>              <span class="hljs-attr">protocol:</span> <span class="hljs-string">TCP</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">containerPort:</span> <span class="hljs-number">9300</span>              <span class="hljs-attr">protocol:</span> <span class="hljs-string">TCP</span>          <span class="hljs-attr">volumeMounts:</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">esdata</span>              <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/usr/share/elasticsearch/data</span>          <span class="hljs-attr">env:</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">"cluster.name"</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">"elasticsearch-cluster"</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">"bootstrap.memory_lock"</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">"true"</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">"discovery.zen.ping.unicast.hosts"</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">"elasticsearch-discovery"</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">"node.master"</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">"false"</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">"node.data"</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">"true"</span>            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">"ES_JAVA_OPTS"</span>              <span class="hljs-attr">value:</span> <span class="hljs-string">"-Xms256m -Xmx256m"</span>          <span class="hljs-attr">securityContext:</span>            <span class="hljs-attr">privileged:</span> <span class="hljs-literal">true</span>      <span class="hljs-attr">volumes:</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">esdata</span>          <span class="hljs-attr">emptyDir:</span> <span class="hljs-string">&#123;&#125;</span>      <span class="hljs-attr">serviceAccountName:</span> <span class="hljs-string">elasticsearch-admin</span>      <span class="hljs-attr">tolerations:</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">key:</span> <span class="hljs-string">node-role.kubernetes.io/master</span>          <span class="hljs-attr">effect:</span> <span class="hljs-string">NoSchedule</span><span class="hljs-meta">---</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Service</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">elastic-app:</span> <span class="hljs-string">elasticsearch-service</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">elasticsearch-service</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">ns-elasticsearch</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">ports:</span>    <span class="hljs-bullet">-</span> <span class="hljs-attr">port:</span> <span class="hljs-number">9200</span>      <span class="hljs-attr">targetPort:</span> <span class="hljs-number">9200</span>  <span class="hljs-attr">selector:</span>    <span class="hljs-attr">elastic-app:</span> <span class="hljs-string">elasticsearch</span>  <span class="hljs-attr">type:</span> <span class="hljs-string">NodePort</span></code></pre><blockquote><p>通过环境变量设置：node.master、node.data和node.ingest来控制对应的角色，三个选项的默认值均是true；</p><p>通过postStart来设置系统内核参数，但是对memlock无效；网上也有通过initPod进行修改的，暂时没有尝试过。</p><p>通过设置discovery.zen.minimum_master_nodes来防止脑裂，表示多少个master候选节点在线时集群可用；</p><p>通过设置discovery.zen.ping_timeout来降低网络延时带来的脑裂现象；</p><p>节点通过单播来发现和加入进群，设置elasticsearch-discovery服务作为进群发现入口；</p><p>serviceAccount是否有必要带验证！</p></blockquote><pre><code class="hljs yaml"><span class="hljs-comment">### 在K8S上发布ES集群 ###</span><span class="hljs-string">[root@k8s-master</span> <span class="hljs-string">elasticsearch-cluster]#</span> <span class="hljs-string">kubectl</span> <span class="hljs-string">apply</span> <span class="hljs-string">-f</span> <span class="hljs-string">elasticsearch-cluster.yml</span></code></pre><h3 id="2-5、-检查并测试集群"><a href="#2-5、-检查并测试集群" class="headerlink" title="2.5、 检查并测试集群"></a>2.5、 检查并测试集群</h3><pre><code class="hljs shell">[root@k8s-master elasticsearch-cluster]# kubectl get pods --all-namespacesNAMESPACE          NAME                                    READY     STATUS    RESTARTS   AGEkube-system        etcd-k8s-master                         1/1       Running   7          5dkube-system        kube-apiserver-k8s-master               1/1       Running   7          5dkube-system        kube-controller-manager-k8s-master      1/1       Running   4          5dkube-system        kube-dns-6f4fd4bdf-mwddx                3/3       Running   10         5dkube-system        kube-flannel-ds-nwxcl                   1/1       Running   5          5dkube-system        kube-flannel-ds-pdxhs                   1/1       Running   5          5dkube-system        kube-flannel-ds-qgbmf                   1/1       Running   3          5hkube-system        kube-flannel-ds-qrkwd                   1/1       Running   3          5dkube-system        kube-proxy-7lsqw                        1/1       Running   5          5dkube-system        kube-proxy-bq6kg                        1/1       Running   0          5hkube-system        kube-proxy-f9kps                        1/1       Running   5          5dkube-system        kube-proxy-g47nx                        1/1       Running   3          5dkube-system        kube-scheduler-k8s-master               1/1       Running   4          5dkube-system        kubernetes-dashboard-845747bdd4-gcj47   1/1       Running   3          5dns-elasticsearch   elasticsearch-master-65c8cc584c-jcsq4   1/1       Running   0          4hns-elasticsearch   elasticsearch-master-65c8cc584c-kwg69   1/1       Running   0          4hns-elasticsearch   elasticsearch-master-65c8cc584c-rdcbz   1/1       Running   0          4hns-elasticsearch   elasticsearch-node-6f9d5fbd6c-6jktr     1/1       Running   0          4hns-elasticsearch   elasticsearch-node-6f9d5fbd6c-x7qk8     1/1       Running   0          4h[root@k8s-master elasticsearch-cluster]# kubectl get svc --all-namespacesNAMESPACE          NAME                      TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGEdefault            kubernetes                ClusterIP   10.96.0.1        &lt;none&gt;        443/TCP          5dkube-system        kube-dns                  ClusterIP   10.96.0.10       &lt;none&gt;        53/UDP,53/TCP    5dkube-system        kubernetes-dashboard      NodePort    10.111.158.221   &lt;none&gt;        443:32286/TCP    5dns-elasticsearch   elasticsearch-discovery   ClusterIP   10.97.150.85     &lt;none&gt;        9300/TCP         1mns-elasticsearch   elasticsearch-service     NodePort    10.101.40.47     &lt;none&gt;        9200:31112/TCP   1m</code></pre><p><img src="C:%5CUsers%5CAdministrator%5CPictures%5C20180306090516400.png" srcset="/img/loading.gif" alt="20180306090516400"></p><blockquote><p>上图中的node.role包含mdi，分别对呀master、data和ingest。</p></blockquote><p><img src="C:%5CUsers%5CAdministrator%5CPictures%5C20180306090539949.png" srcset="/img/loading.gif" alt="20180306090539949"></p><h2 id="3、-注意事项"><a href="#3、-注意事项" class="headerlink" title="3、 注意事项"></a>3、 注意事项</h2><ul><li>如果k8s节点内存不够，可能导致部分ES节点报错</li><li>集群发现通过k8s service来实现，ES本身“discovery.zen.ping.unicast.hosts”也不需要配置所有节点列表，新节点只需要连上集群中任意一个有效节点即可找到全部</li><li>ulimit -l unlimited无法在postStart中生效</li><li>在dashboard中可以比较方便的查看日志，当然也可以使用kubectl log命令，查看日志可以快速定位问题</li></ul><p><img src="C:%5CUsers%5CAdministrator%5CPictures%5C20180306090616966.png" srcset="/img/loading.gif" alt="20180306090616966"></p><ul><li>尝试增加数据节点</li></ul><pre><code class="hljs shell">[root@k8s-master ~]# kubectl scale deployment elasticsearch-data --replicas 3 -n ns-elasticsearch[root@k8s-master ~]# kubectl get pods --all-namespacesNAMESPACE          NAME                                    READY     STATUS    RESTARTS   AGEkube-system        etcd-k8s-master                         1/1       Running   8          5dkube-system        kube-apiserver-k8s-master               1/1       Running   8          5dkube-system        kube-controller-manager-k8s-master      1/1       Running   5          5dkube-system        kube-dns-6f4fd4bdf-mwddx                3/3       Running   13         5dkube-system        kube-flannel-ds-nwxcl                   1/1       Running   6          5dkube-system        kube-flannel-ds-pdxhs                   1/1       Running   6          5dkube-system        kube-flannel-ds-qgbmf                   1/1       Running   4          21hkube-system        kube-flannel-ds-qrkwd                   1/1       Running   4          5dkube-system        kube-proxy-7lsqw                        1/1       Running   6          5dkube-system        kube-proxy-bq6kg                        1/1       Running   1          21hkube-system        kube-proxy-f9kps                        1/1       Running   6          5dkube-system        kube-proxy-g47nx                        1/1       Running   4          5dkube-system        kube-scheduler-k8s-master               1/1       Running   5          5dkube-system        kubernetes-dashboard-845747bdd4-gcj47   1/1       Running   4          5dns-elasticsearch   elasticsearch-data-657bb6c8f9-gpv89     1/1       Running   0          9sns-elasticsearch   elasticsearch-data-657bb6c8f9-jbrkc     1/1       Running   1          15hns-elasticsearch   elasticsearch-data-657bb6c8f9-pzmj5     1/1       Running   1          15hns-elasticsearch   elasticsearch-master-976859f58-qltf5    1/1       Running   1          15hns-elasticsearch   elasticsearch-master-976859f58-ts65d    1/1       Running   1          15hns-elasticsearch   elasticsearch-master-976859f58-zwqgv    1/1       Running   1          15h</code></pre><p><img src="C:%5CUsers%5CAdministrator%5CPictures%5C20180306090642780.png" srcset="/img/loading.gif" alt="20180306090642780"></p><blockquote><p>上图出现新节点可能需要等待一分钟左右</p></blockquote><h2 id="4、附件下载"><a href="#4、附件下载" class="headerlink" title="4、附件下载"></a>4、附件下载</h2><p>网盘地址：<a href="https://pan.baidu.com/s/1PN0ku0BNYWZUI_5moXS9cA" target="_blank" rel="noopener">https://pan.baidu.com/s/1PN0ku0BNYWZUI_5moXS9cA</a></p><blockquote><p>解压：</p><pre><code class="hljs shell">[root@k8s-master elasticsearch-cluster]# tar -zxvf elasticsearch-6.2.2.tar.gzelasticsearch-6.2.2.tar</code></pre><p>导入：</p><pre><code class="hljs shell">[root@k8s-master elasticsearch-cluster]# docker load -i elasticsearch-6.2.2.tar</code></pre></blockquote><h3 id="5、-参考资料"><a href="#5、-参考资料" class="headerlink" title="5、 参考资料"></a>5、 参考资料</h3><ul><li><a href="http://blog.csdn.net/zwgdft/article/details/54585644" target="_blank" rel="noopener">http://blog.csdn.net/zwgdft/article/details/54585644</a></li><li><a href="http://blog.csdn.net/zwgdft/article/details/54585644" target="_blank" rel="noopener">http://blog.csdn.net/zwgdft/article/details/54585644</a></li><li><a href="http://blog.csdn.net/a19860903/article/details/72467996" target="_blank" rel="noopener">http://blog.csdn.net/a19860903/article/details/72467996</a></li><li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/6.2/docker.html" target="_blank" rel="noopener">https://www.elastic.co/guide/en/elasticsearch/reference/6.2/docker.html</a></li><li><a href="https://github.com/cesargomezvela/elasticsearch" target="_blank" rel="noopener">https://github.com/cesargomezvela/elasticsearch</a></li><li><a href="https://github.com/kubernetes/kubernetes/issues/3595" target="_blank" rel="noopener">https://github.com/kubernetes/kubernetes/issues/3595</a></li></ul><p>————————————————<br>版权声明：本文为CSDN博主「迷途的攻城狮（798570156）」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。<br>原文链接：<a href="https://blog.csdn.net/chenleiking/java/article/details/79453460" target="_blank" rel="noopener">https://blog.csdn.net/chenleiking/java/article/details/79453460</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>IT</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
